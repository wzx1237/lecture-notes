{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM16Qc7LhH7HceKM66MuqAV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["10 月 14 号\n","\n","##Artificial Neural Network\n","\n","**Image recognition and classification** is a subdomain of computer vision. It is an algorithm that looks at an image and **assigns it a label** from a collection of predefined labels or categories\n","\n","什么是ANN？\n","1. Artificial neural networks (ANN) are one of the **most powerful  artificial intelligence and machine learning algorithms.**\n","2. An ANN is considered a **universal function approximator** that transforms inputs into outputs.\n","3. As the name suggests, it **draws inspiration from neurons in our brain** and the way they are connected.\n","\n","例如：\n","\\begin{matrix}\n","&Biological \\quad Neuron    \\quad        &Artificial \\quad Neuron \\\\\n","&树突                           &inputs \\\\\n","&细胞体                         &node \\\\\n","&轴突                           &output \\\\\n","&Synapse                       &weight\n","\\end{matrix}\n","\n","## What is an Artificial Neuron?\n","1. An artificial neuron is a simple biological neuron model in an artificial neural network.\n","2. It performs certain calculations to detect input data capabilities.\n","3. One of the most simplest models of neuron is called perceptron (or threshold logic unit).\n","\n","**An example of Artificial Neuron**\n","\n","Neuron = a linear function & an activation function\n","\\begin{align}\n","output = f(\\omega_{1} \\times x_{1} + \\omega_{2} \\times x_{2} + \\theta)\n","\\end{align}\n","As this example, linear combination is: $(\\omega_{1} \\times x_{1} + \\omega_{2} \\times x_{2} + \\theta)$, and the activation funcation is: $f$.\n","\n","我们有很多种不同的weight, $\\omega$ 和activation function, $f$.\n","\n","例如：\n","\\begin{align}\n","f(x) = \\frac{1}{1 + e^{-x}}\n","\\end{align}\n","\n","$$f(x) = \\left\\{\n","\\begin{aligned}\n","   &1, x > 0 \\\\\n","  -&1, x \\le 0\n","\\end{aligned}\n","\\right.\n","$$"],"metadata":{"id":"Pm-NKJjXwPir"}},{"cell_type":"markdown","source":["##Perception Learing rules\n","\n","公式：\n","\n","$$\\Delta \\omega_{i}  = \\eta (T-O)x_{i}$$\n","$$\\Delta \\theta = \\eta (T-O)$$\n","$$\\omega_{i} = \\omega_{i} + \\Delta \\omega_{i}$$\n","$$\\theta = \\theta + \\Delta \\theta$$\n","\n","Here : $O$ represents \"output\", $T$ is label and $(T-O)$ is the error.\n","\n","$\\theta$ is bias, $\\omega$ is weight, $x_{i}$ is input, $\\eta$ is learing rate.\n","\n","注意：这个公式只对perception生效（即：activity function是sign function）\n","其他function不要用这个公式\n","\n","##Convergence Criterion\n","当我们的model在给定的training data上不再出错时，我们的训练结束(converge)\n","\n","##Stopping Rules\n","1. Use maximum **training time**\n","2. The maximum **number of training** cycles allowed\n","3. Use minimum **change of accuracy**\n","\n","**Why do not convergence for our stopping criterion?**\n","\n","Because we may not achieve converge (e.x. non-linear data cannot converge by using Perception model)"],"metadata":{"id":"RcZo3wCL5b49"}},{"cell_type":"code","source":["import math\n","class Perception:\n","  def __init__(self):\n","    self.w = [0.1, 0.5]     # \\omega_{i}\n","    self.theta = -0.8       # \\theta\n","    self.learningRate = 0.2 # \\eta\n","  def response(self,x):\n","    \"\"\" Perceptron output \"\"\"\n","    # Calculate weighted sum\n","    y = x[0] * self.w[0] + x[1] * self.w[1] + self.theta\n","    # If weighted sum >= 0, return 1. Otherwise return 0\n","    # f(x):\n","    if y >= 0:\n","      return 1\n","    else:\n","      return 0\n","  def updateWeights(self,x,iterError): # iterError: T- O\n","    \"\"\" Weights update \"\"\"\n","    # wi = wi + eta * (T-O) * xi\n","    self.w[0] += self.learningRate * iterError * x[0]\n","    self.w[1] += self.learningRate * iterError * x[1]\n","  def updateBias(self,iterError):\n","    \"\"\" Bias update \"\"\"\n","    # theta = theta + eta * (T-O)\n","    self.theta += self.learningRate * iterError\n","  def train(self,data):\n","    \"\"\" Training \"\"\"\n","    learned = True # Should perform training\n","    round = 0      # Initialize round to 0\n","    while learned:\n","      totalError = 0.0\n","      # 这个for loop是在iter所有的training data\n","      for x in data:\n","        r = self.response(x)\n","        if x[2] != r:                       # T - O != 0\n","          roundError = x[2]- r\n","          self.updateWeights(x,roundError)  # w_i = w_i + eta * (T-O) * xi\n","          self.updateBias(roundError)       # theta = theta + eta * (T-O)\n","          totalError += abs(roundError)\n","      round += 1\n","\n","      # round: 最大循环次数\n","      if math.isclose(totalError, 0) or round >= 100:\n","        print(\"Total number of rounds (epochs): \", round)\n","        print(\"Final weights: \", self.w)\n","        print(\"Final bias: \", self.theta)\n","        learned = False"],"metadata":{"id":"xLFHa2Ef898M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  perception = Perception()\n","  trainset = [[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 1]]\n","  perception.train(trainset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhcqZhok-xtd","executionInfo":{"status":"ok","timestamp":1760584727177,"user_tz":-480,"elapsed":30,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"c4347955-156a-4084-e41b-622d2a16aed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of rounds (epochs):  3\n","Final weights:  [0.30000000000000004, 0.49999999999999994]\n","Final bias:  -0.8\n"]}]},{"cell_type":"markdown","source":["##Remark:\n","1. The weight and bias are not unique. For example, if we give it different initialization, the output would be different. (SKlearn perception would ramdomly initialize all parameter for you, except learing rate.)\n","2. what is learning rate? $\\eta$ is larger, the change of weights($\\omega_{i}$) and bias($\\theta$) would be fast, 'learing' is faster; conversely, $\\eta$ is smaller, the change of weights and bias would be slow, 'learing' would be slower.\n","3. Some terminology: **Learning** AND **Epoch**\n","\n","    Learning: process of updating weights in the perceptron.\n","\n","    Epoch refers to one cycle through the full training dataset.\n","\n","##Pros and Cons with different learning rate\n","1. when the learning rate $\\eta$ is big: learning is fast, but might hard to converge. (例如在minimum附近震荡)\n","2. when the learning rate $\\eta$ is small: learning is slow, and easy converge to local minimum."],"metadata":{"id":"9QI40RUjAPIC"}},{"cell_type":"markdown","source":["##Decision Boundary\n","EXAMPLE:\n","\n","if we get our training result: $\\omega_{1} = 0.3$, $\\omega_{2} = 0.5$, $\\theta = -0.8$\n","\n","$$y = \\left\\{\n","  \\begin{aligned}\n","  &0, 3x_{1} + 5x_{2} < 8 \\\\\n","  &1, 3x_{1} + 5x_{2} ≥ 8\n","  \\end{aligned}\n","\\right.\n","$$\n","\n","The straight line defined by $3x_1 + 5x_2 = 8$ is called Decision Boundary of the perceptron.\n","\n","**Decision Boundary:**\n","\n","In a **binary classification problem**, a decision boundary or decision surface is a hypersurface that **partitions** the underlying vector space **into two sets**, one for each class. The classifier will **classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.**"],"metadata":{"id":"j6F34dllFjv4"}},{"cell_type":"markdown","source":["##Shortage\n","Question: Can we apply the same perceptron learning\n","procedure for the XOR gate, which has the truth table on\n","the right? If so, show all the steps. If not, explain why."],"metadata":{"id":"vPW_KXSAHvZp"}}]}