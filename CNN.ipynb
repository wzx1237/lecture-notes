{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMcJRtH4JpdTSVq+DI64u6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#CNN\n","\n","**Convolutional Neural Network** (CNN or ConvNet) is a type of Artificial Neural Networks applied to **analyze visual imagery**.\n","\n","研究表示：图像处理(image processing)能显著提高CNN的分类准确率\n","\n","##Digital Image Processing\n","\n","什么是图像处理？\n","\n","**Digital image processing** (DIP) is the method to manipulate a digital image to either **enhance the quality or extract relevant information**.\n","\n","什么是digital image?\n","\n","A digital image is a **two-dimensional grid of intensity values**, represented by $I(x,y)$, where x and y are coordinates, and the value of $I$ at coordinates $(x,y)$ is called **intensity**.\n","\n","**Pixels:** Short for Picture Element. A pixel is a single point (dot) in an image.\n","\n","**Dimensions:** Specified by the width and height of the image.\n","1. Image width is the number of columns in the image.\n","2. Image height is the number of rows in the image.\n","\n","A specific pixel is specified by its coordinates $(x,y)$ where $x$ is increasing from left to right, and $y$ is increasing from top to bottom.The origin $(0,0)$ is in the top-left corner.\n","\n","例子：\n","\\begin{matrix}\n","&(0, 0)  &(1, 0) &...  &(width-1,0) \\\\\n","&(0, 1)  &(1, 1) &...  &(width-1,1) \\\\\n","&...     &...    &...  &... \\\\\n","&(0, height - 1)  &(1, height - 1) &...  &(width-1,height - 1)\n","\\end{matrix}\n","\n","**Grayscale Images:**\n","Grayscale is a range of gray shades from black to white.\n","\n","**Range:** 0 ~ 255 (0 for black, 127 for gray, and 255 for white)\n","\n","在数据处理时，我们经常使用Grayscale，因为Grayscale的数据量小，处理起来比较方便快捷。\n","\n","**Color image:**\n","Color images have **intensity** from the darkest and lightest of 3 different colors, Red, Green, and Blue **(RGB)**.\n","\n","例子：\n","\n","| Color | RGB value |\n","|:----- | :-----    |\n","|Black  | RGB = (0, 0, 0)|\n","|White  | RGB = (255, 255, 255)|\n","|Red    | RGB = (255, 0, 0)|\n","|Green  | RGB = (0, 255, 0)|\n","|Blue   | RGB = (0, 0, 255)|\n","|Yellow | RGB = (255, 255, 0)|\n"],"metadata":{"id":"atE72KXk6sHV"}},{"cell_type":"code","source":["# Access images in Google drive\n","\n","# Import drive from google.colab package\n","from google.colab import drive\n","# Import os and sys modules\n","import os, sys\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","# Assume a folder \"images\" has been created, go to the folder \"images\"\n","os.chdir('/content/drive/My Drive/images')\n","# Add the path for interpreter to search\n","sys.path.append('/content/drive/My Drive/images')"],"metadata":{"id":"CrzMYvm-L3br"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read Images in Colab\n","\n","# import library:\n","import matplotlib.image as mpimg\n","\n","# 使用imread()来读取这个image\n","# 格式：\n","# mpimg.imread(fname, format=none)\n","# fname: 读取文档或者照片的名字\n","# format: 读取文档或者的格式，如果不提供format, python会self-deduct,\n","#         如果self-deduct失败，会默认尝试使用png格式打开\n","# return type: numpy.array\n","# array.shape:\n","#     (M, N) for Grayscale image\n","#     (M, N, 3) for RGB image\n","#     (M, N, 4) for RGBA images: red, green, blue, alpha, 这里alpha指示了每一个像素的透明度\n","# PNG images are returned as float_32 arrays (0-1). (usually)\n","# All other formats are returned as int_8 arrays (usually), with a bit depth determined by the file’s contents."],"metadata":{"id":"92b88nPZMSLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show image in Colab\n","import matplotlab.pyplot as plt\n","\n","# 使用imshow()来展示这个图像：\n","# 格式：\n","# plt.imshow(X, cmap, vmin, vmax)\n","\n","# Parameters:\n","# X: The image data.\n","#    The first two dimensions (M,N) define the rows and columns of the image.\n","\n","# cmap: str (e.g., ’gray’) or Colormap. The Colormap instance or registered colormap name used to map\n","#       scalar data to colors. This parameter is ignored for RGB(A) data.\n","\n","# vmin, vmax:\n","#     By default, imshow scales elements of the numpy array so that the smallest element becomes 0,\n","#     the largest becomes 1, and intermediate values are mapped to the interval [0,1] by a linear\n","#     function.\n","#     Optionally, imshow can be called with arguments, vmin and vmax. In such case all elements of the\n","#     array smaller or equal to vmin are mapped to 0, all elements greater or equal to vmax are sent to\n","#     1, and the elements between vmin and vmax are mapped in a linear fashion into the interval [0,1].\n","\n","# Returns AxesImage\n","# AxesImage is an image attached to an Axes.\n"],"metadata":{"id":"XE8uKTjMA8H1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save image:\n","\n","# 使用imsave()\n","# 格式：\n","# plt.imsave(fname, arr)\n","# Parameters:\n","# fname: 存储的位置/文档名字\n","# arr: image array（图片本身）"],"metadata":{"id":"x9MPuvxvCh_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#OpenCV\n","\n","##Image Processing using OpenCV\n","\n","**OpenCV (Open Source Computer Vision Library)**: is an open source computer vision and machine learning software library.\n","\n","##Convert Color Images to Grayscale\n","如何将一个图片变成黑白图片？\n","\n","Ony way to convert color images to grayscale is to apply the following **formula**:\n","\\begin{align}\n","  V = 0.299 \\times R + 0.587 \\times G + 0.114 \\times B\n","\\end{align}\n","如何在OpenCV上实现？\n","\n"],"metadata":{"id":"r-IKC4tYDLBq"}},{"cell_type":"code","source":["# convert color image to grayscale\n","import cv2\n","\n","# 使用：cvtColor()\n","# 格式：\n","# cv2.cvtColor(image, code)\n","# image: Image to be processed in n-dimensional array\n","# code: Conversion code for colorspace. For converting RGB to grayscale, we use cv2.COLOR_RGB2GRAY"],"metadata":{"id":"XFNOE2sDE1dq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Image Affine Transformation\n"," An affine transformation is any **transformation** that **preserves collinearity, parallelism as well as the ratio of distances between the points**\n","\n"," Affine transformation不会保证角度和距离不变\n","\n"," 旋转，放缩，平移都属于affine transformation\n","\n","In general, the affine transformation can be expressed in the form of a **linear transformation followed by a vector addition** as follows:\n","$$\\left(\\begin{matrix} x_1 \\\\ y_1 \\end{matrix}\\right)\n","= \\left(\\begin{matrix} a_{00} & a_{01} \\\\ a_{10} & a_{11} \\end{matrix}\\right)\n","\\left(\\begin{matrix} x_0 \\\\ y_0 \\end{matrix}\\right) +\n","\\left(\\begin{matrix} b_0 \\\\ b_1 \\end{matrix}\\right)$$\n","\n","So:\n","$$\\left(\\begin{matrix} x_1 \\\\ y_1 \\end{matrix}\\right) =\n","\\left(\\begin{matrix} a_{00} & a_{01} &b_0\\\\ a_{10} & a_{11} &b_1 \\end{matrix}\\right) \\left(\\begin{matrix} x_0 \\\\ y_0 \\\\ 1 \\end{matrix}\\right)$$\n","\n","这里，我们将矩阵$M = \\left(\\begin{matrix} a_{00} & a_{01} &b_0 \\\\ a_{10} & a_{11} &b_1 \\end{matrix}\\right)$称为：**transformation matrix.**\n"],"metadata":{"id":"ndMEXzkHFOwy"}},{"cell_type":"markdown","source":["## Image Translation\n","Translation is simply the shifting of object location.\n","\n","**Transformation Matrix**:\n","$$M = \\left(\\begin{matrix} 1 & 0 &b_0 \\\\ 0 & 1 &b_1 \\end{matrix}\\right)$$\n","\n","##Image Reflection\n","Image reflection的本质是：\n","1. 将图片沿x/y-axis filp\n","2. 将filp后的图像再平移下来\n","\n","**Transformation Matrix**\n","\n","respect to x-axis:\n","$$M = \\left(\\begin{matrix} 1 & 0 &0 \\\\ 0 & -1 &rows - 1 \\end{matrix}\\right)$$\n","\n","respect to y-axis:\n","$$M = \\left(\\begin{matrix} -1 & 0 &cols - 1 \\\\ 0 & 1 & 0 \\end{matrix}\\right)$$"],"metadata":{"id":"d4vX-ewbKEij"}},{"cell_type":"markdown","source":["##Image Rotation\n","Image rotation：绕点$(x_0, y_0)$旋转$\\theta$度\n","\n","**Formula:**\n","\\begin{align}\n","x_{1} &= (x−x_0)cosθ + (y − y_0)sinθ + x_0 \\\\\n","y_{1} &= −(x−x_0)sinθ + (y − y_0)cosθ + y_0\n","\\end{align}\n","\n","**Transformation Matrix**:\n","$$\\left(\\begin{matrix} x_1 \\\\ y_1 \\end{matrix} \\right) =\n","\\left(\\begin{matrix} cos\\theta &sin\\theta &-x_0cos\\theta - y_0sin\\theta +x_0 \\\\ -sin\\theta &cos\\theta &x_0sin\\theta - y_0cos\\theta +y_0 \\end{matrix} \\right)\n","\\left(\\begin{matrix} x \\\\ y \\\\ 1 \\end{matrix} \\right)$$"],"metadata":{"id":"vv6POz68M4rO"}},{"cell_type":"code","source":["import cv2\n","# Image Translation:\n","# 使用 cv2.warpAffine\n","# 格式：cv2.warpAffine(src, M, dsize, flags, borderMode, borderValue)\n","\n","# Input Parameter:\n","# src: input image\n","# M: 2×3 transformation matrix\n","# dsize: size of the output image\n","# flags: combination of interpolation methods\n","# borderMode: ...\n","# borderValue: ...\n","\n","# Return Type:\n","# 同src一样的data type"],"metadata":{"id":"tHWZV1QEa9fI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Image Resize\n","适用于对图片进行拉伸或缩放"],"metadata":{"id":"h7VlEP5pcYKC"}},{"cell_type":"code","source":["import cv2\n","# 使用：cv2.resize\n","# 格式：cv2.resize(src, dsize, dst, fx = 0, fy = 0, interpolation = INTER_LINEAR)\n","\n","# Input Parameter:\n","# src: Input image\n","# dsize: The size for the output image\n","# dst (optional): The output image with size dsize\n","# fx (optional): The scale factor along the horizontal axis\n","# fy (optional): The scale factor along the vertical axis\n","# interpolation: ...\n","\n","# Return Type:\n","# 同src"],"metadata":{"id":"cqW8y_5AckiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Characteristics of Image Operations\n","1. **Point**: The output value at a specific coordinate is dependent **only on the input value at the same coordinate**.\n","2. **Local**: The output value at a specific coordinate is dependent on the input values in the **neighborhood of that same coordinate**.\n","3. **Global**: The output value at a specific coordinate is dependent on **all the values in the input image**."],"metadata":{"id":"YgD9a0MGP_-q"}},{"cell_type":"markdown","source":["##Contrast Streching\n","**Contrast stretching** is an image enhancement method which attempts to improve an image by **stretching the range of intensity values**.\n","\n","要点：增加图片的对比度\n","\n","**Formula:**\n","\\begin{align}\n","I_{new} = \\frac{I−I_{min}}{I_{max} −I_{min}}×255\n","\\end{align}\n","\n","You can consider it as a kind of **'re-normalization'**, sending minimum intensity to 0, and miximum intensity to 255.\n","\n","##Gary thershold: Otsu’s method\n","\n","It is a way to create a binary image from a grayscale to or full-color image\n","\n","$$I_{new} = \\left\\{\\begin{aligned} 0, &I < T \\\\\n","255, &otherwise \\end{aligned}\\right.$$\n","\n","We need a way to automatically determine the threshold value $T$ so that the result of thresholding is reproductible.\n","1. Select an initial estimate of the threshold $T$. A good initial value is the average intensity of the image.\n","2. **Partition the image into two groups**, $R_1$, $R_2$, using the threshold $T$.\n","3. Calculate the mean gray values $\\mu_1$ and $\\mu_2$ of the partitions, $R_1$, $R_2$.\n","4. Compute a new threshold\n"," $$T = \\frac{1}{2} (\\mu_1 + \\mu_2)$$\n","5. Repeat steps 2-4 until the mean values $\\mu_1$ and $\\mu_2$ in successive iterations do not change.\n","\n","**Remark**:\n","the initial value would **not effect the final result**, just would effect the echo we need for train."],"metadata":{"id":"WhBdhJyqQxv_"}},{"cell_type":"code","source":["# Ostu's Method\n","import cv2\n","# 使用：cv2.threshold()\n","# 格式：cv2.threshold(source, thresholdValue, maxVal, thresholdingTechnique)\n","\n","# Input Parameter\n","# source: input image array (must be grayscale)\n","# thresholdValue: value of threshold below and above which pixel values will change accordingly\n","# maxVal: Maximum value that can be assigned to a pixel\n","# thresholdingTechnique: The type of thresholding to be applied\n","# (For Otsu’s, we put cv2.THRESH BINARY + cv2.THRESH OTSU)\n","\n","# Return Type:\n","# 有两个返回值：\n","# 第一个是：使用的thresholding Technique\n","# 第二个是：处理完的图像"],"metadata":{"id":"X434MZ1gdV9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Convolution\n","A convolution is an **integral that expresses the amount of overlap** of one function $g$ as it is shifted over another function $f$. It therefore “blends” one function with another.\n","\n","计算方式：\n","\n","Image convolution is defined as\n","\\begin{align}\n"," O(x,y) =\\sum_{m=−∞}^{∞}\\sum_{n=−∞}^{∞}K(m,n)I(x−m,y −n)\n","\\end{align}\n"," where I is the input image, K is the image kernel.\n","\n","Assume the origin (i.e., $(0,0)$) of I is top-left corner, while\n","the origin (i.e., $(0,0)$) of $K$ is the center of the kernel.\n","If the image kernel is $3×3$, then\n","\\begin{align}\n"," O(x,y) =\\sum_{m=−1}^{1}\\sum_{n=−1}^{1}K(m,n)I(x−m,y −n)\n","\\end{align}\n","\n","例子：\n","\n","Input Image\n","$$I = \\left(\n","\\begin{matrix}\n","10 &1 &3 &2 &6 \\\\\n","4 &3 &5 &8 &0  \\\\\n","8 &7 &9 &6 &5\n","\\end{matrix}\\right)$$\n","\n","Image Kernel:\n","$$K = \\left(\\begin{matrix}\n","-1 &0 &1 \\\\\n","-1 &0 &1 \\\\\n","-1 &0 &1 \\end{matrix}\\right)$$\n","\\begin{align}\n","O(1,1)\n"," =&K(−1,−1)I(2,2) +K(−1,0)I(2,1) + K(−1,1)I(2,0)+ \\\\\n"," &K(0,−1)I(1,2) + K(0,0)I(1,1) + K(0,1)I(1,0)+     \\\\\n"," &K(1,−1)I(0,2) + K(1,0)I(0,1) + K(1,1)I(0,0) \\\\\n"," &=(−1)(9) + (−1)(5) + (−1)(3) +(0)(7) + (0)(3)\n"," + (0)(1) + (1)(8) + (1)(4) + (1)(10) \\\\\n"," &=−9−5−3+8+4+10=5\n","\\end{align}\n","\n","取巧的计算方法：\n","1. 将kernel左右翻转后再上下翻转\n","2. 将翻转完成后的inverse kernel与原图像进行memberwise的乘法"],"metadata":{"id":"TvNhlRhR0A71"}},{"cell_type":"markdown","source":["##Problem: boundary pixel\n","##Method 1: Zero Padding\n","我们可以再原图像的边缘镶上一圈0，这样boundary pixel就可以被处理了\n","\n","例：\n","$$I_{new} = \\left(\n","\\begin{matrix}\n","0 &0 &0 &0 &0 &0 &0 \\\\\n","0 &10 &1 &3 &2 &6 &0 \\\\\n","0 &4 &3 &5 &8 &0 &0  \\\\\n","0 &8 &7 &9 &6 &5 &0 \\\\\n","0 &0 &0 &0 &0 &0 &0\n","\\end{matrix}\\right)$$\n","\n","##Method 2: Mirror Padding\n","我们可以将原图像pixel沿边缘做一个镜像对称，这样就得到了mirror padding.\n","例：\n","$$I_{new} = \\left(\n","\\begin{matrix}\n","3 &4 &3 &5 &8 &0 &8 \\\\\n","1 &10 &1 &3 &2 &6 &2 \\\\\n","3 &4 &3 &5 &8 &0 &8  \\\\\n","7 &8 &7 &9 &6 &5 &6 \\\\\n","3 &4 &3 &5 &8 &0 &8\n","\\end{matrix}\\right)$$\n","\n","##Method 3: Reflection Padding\n","和Mirror Padding类似，但是对称轴不同。\n","\n","**注：在padding = 1的情况下，它和Replication padding会产生一样的结果。但是当padding > 1时， 二者的结果不同。**\n","\n","例：padding = 2\n","$$I_{new} = \\left(\n","\\begin{matrix}\n","3 &4  &4  &3 &5 &8 &0 &0 &8 \\\\\n","1 &10 &10 &1 &3 &2 &6 &6 &2 \\\\\n","1 &10 &10 &1 &3 &2 &6 &6 &2 \\\\\n","3 &4  &4  &3 &5 &8 &0 &0 &8 \\\\\n","7 &8  &8  &7 &9 &6 &5 &5 &6 \\\\\n","7 &8  &8  &7 &9 &6 &5 &5 &6 \\\\\n","3 &4  &4  &3 &5 &8 &0 &0 &8\n","\\end{matrix}\\right)$$\n","\n","##Method 4: Replication Padding\n","复制boundary pixel\n","\n","例：padding = 2\n","\n","$$I_{new} = \\left(\n","\\begin{matrix}\n","10 &10 &10 &1 &3 &2 &6 &6 &6 \\\\\n","10 &10 &10 &1 &3 &2 &6 &6 &6 \\\\\n","10 &10 &10 &1 &3 &2 &6 &6 &6 \\\\\n","4 &4  &4  &3 &5 &8 &0 &0 &0 \\\\\n","8 &8  &8  &7 &9 &6 &5 &5 &5 \\\\\n","8 &8  &8  &7 &9 &6 &5 &5 &5 \\\\\n","8 &8  &8  &7 &9 &6 &5 &5 &5\n","\\end{matrix}\\right)$$"],"metadata":{"id":"x63G1zIC44Hw"}},{"cell_type":"code","source":["# Implementation in OpenCV\n","import cv2\n","# using filter2D():\n","# 格式：\n","# cv2.filter2D(src, ddepth, kernel, dst, anchor, delta, borderType=cv2.BORDER_DEFAULT)\n","# parameters:\n","# src: 原图像\n","# ddepth: 想要的图片深度\n","# kernel: convolution kernel\n","# dst:\n","# anchor:"],"metadata":{"id":"jCgne1we-ceE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Smoothing Kernel\n","$$I = \\left(\n","  \\begin{matrix}\n","  \\frac{1}{9} &\\frac{1}{9} &\\frac{1}{9} \\\\\n","  \\frac{1}{9} &\\frac{1}{9} &\\frac{1}{9} \\\\\n","  \\frac{1}{9} &\\frac{1}{9} &\\frac{1}{9} \\end{matrix}\\right) $$\n","显然，这个Kernel会将Intensity变为它周围pixel Intensity的平均值，\n","因而图片变得smooth了\n","##Sharping Kernel\n","$$I = \\left(\n","  \\begin{matrix}\n","  -1 &-1 &-1 \\\\\n","  -1 &9 &-1 \\\\\n","  -1 &-1 &-1 \\end{matrix}\\right) $$\n","为什么它能使图片对比度增强？\n","1. 当core和周围的Intensity相近时，sum会趋近于1，代表没有变化\n","2. 当core和周围的Intensity相差很多时，sum会大于原来的Intensity，\n","\n","所以原来对比小的地方没有变化，而原来对比度大的地方对比被进一步放大了\n","##Edge Kernel\n","\n","**Vertical Edge Kernel**:\n","$$I = \\left(\n","  \\begin{matrix}\n","  -1 &0 &1 \\\\\n","  -1 &0 &1 \\\\\n","  -1 &0 &1 \\end{matrix}\\right) $$\n","为什么它能够探测vertical line?\n","1. 当左右两侧的Intensity相近时，sum会趋近于0\n","2. 当左右两侧的Intensity相差很远时，sum不为0，故\"提取\"出来了vertical edge.\n","\n","**Horizontal Edge Kernel**:\n","$$I = \\left(\n","  \\begin{matrix}\n","  -1 &-1 &-1 \\\\\n","  0 &0 &0 \\\\\n","  1 &1 &1 \\end{matrix}\\right) $$\n","为什么它能够探测horizontal line?\n","1. 当上下两侧的Intensity相近时，sum会趋近于0\n","2. 当上下两侧的Intensity相差很远时，sum不为0，故\"提取\"出来了horizontal edge."],"metadata":{"id":"1OMvskLnANjC"}},{"cell_type":"markdown","source":["#Image Convolutions\n","Clearly, image convolution is powerful in **finding the features of an image** if we already know the right kernel to use.\n","\n","Kernel design is an art and has been refined over the last few decades to do some pretty amazing things with images. But the important question is, what if **we don’t know the features we are looking for**? Or what if we do know, but **we don’t know what kernel should look like**?\n","\n","Convolutional neural networks were developed in the late 1980s and then forgotten about **due to the lack of processing power**.\n","With the powerful Graphical Processing Units (GPUs), the research on CNNs and deep learning was given new life.\n","\n","##什么是CNN?\n","\n","A convolutional neural network is a **neural network with a convolution operation** in at least one of the layers.\n","\n","它是如何运作的？\n","1. 每一个**像素点都是一个feature**，他们就像是ANN里的input node一样\n","2. **kernel就是ANN中的weights**，它们就是在CNN中我们要learn的training parameter\n","3. **result from each convolution就是下一个hidden layer的input**. Each feature or pixel of the convolved image is a node in the hidden layer.\n","\n","在CNN中有3中不同的layer：\n","1. Pooling layer\n","2. Input layer\n","3. Convolutional layer\n"],"metadata":{"id":"dIONvd0ZB1IS"}},{"cell_type":"markdown","source":["##Convulotional Layer\n","**How to calculate output shape and number of paramter?**\n","\n","公式：\n","\n","$\\text{input shape} = (h_1, w_1, c_1)$, $\\text{kernel shape} = (h_2, w_2, c_1)$\n","\n","\\begin{align}\n","\\text{#output shape} &= \\text{output height} \\times \\text{output width} \\times \\text{number of channel} \\\\\n","&= \\frac{h_1 - h_2 + 2 \\times \\text{padding}}{\\text{stride}} \\times \\frac{w_1 - w_2 + 2 \\times \\text{padding}}{\\text{stride}} \\times \\text{number of node in this layer}\n","\\\\ \\\\\n","\\text{#param} &= (h_2 \\times w_2 \\times c_1 + 1)\\times \\text{number of nodes in this layer}\n","\\end{align}\n","\n","**Note that: in the formula that calculate number of parameters, \"$+1$\" is for bias in each node.** And $(h_2 \\times w_2 \\times c_1 + 1)$ represents number of parameters inside each kernel.\n","\n","注意：当除完stride发现不是整数时，选择向下取整(floor)"],"metadata":{"id":"uP0siQ9gNNbZ"}},{"cell_type":"markdown","source":["##Stride\n","Stride is the amount of movement between applications of the kernel to the input image.\n","\n","**定义：**\n","\n","你在进行Convulotion时，可以跳掉一些pixel，这称作stride.\n","\n","为什么我们需要stride?\n","1. 这样可以缩减output volume\n","2. **Lesser memory** needed for output.\n","3. It **avoids overfitting** especially in case of image\n","   processing having a large number of attributes.\n","\n","**Useful Formula for Determining the Size of Output Image**\n","$$\\text{output size} = \\frac{\\text{size of image deminsion} - \\text{size of kernel deminsion} + 2 * \\text{padding}}{\\text{stride}} + 1 $$\n","\n","例子：\n","\n","image: 7 × 7, kernel: 3 × 3\n","Output size:\n","1. Stride 1 ⇒ Output size = (7- 3)/1 + 1 = 5\n","2. Stride 2 ⇒ Output size = (7- 3)/2 + 1 = 3\n","3. Stride 3 ⇒ Output size = (7- 3)/3 + 1 = 2.33 **(Does not fit!)**\n","\n","\n"],"metadata":{"id":"7-gaiTO_Om1p"}},{"cell_type":"markdown","source":["##Non-linearity in CNN\n","Convolution is a **linear operation**. We **need non-linearity**; otherwise, 2 convolution layers would be no more powerful than 1.\n","\n","**Remark:**\n","\n","**Why Convolution is linear operation?**\n","\n","1. Consider a single pixel, in layer 1, it is **a linear\n","   combination of itself and nearby pixel**. So, it is linear operayion.\n","2. Consider layers, each layer is 'linear'. If there is no    non-linear activation function, the whole CNN network is just as a huge linear combination. (i.e. same as 1 layer CNN)\n","\n","In a neural network, we use different activation functions, typically using sigmoid. But rectified linear unit **(ReLU) is more popular for CNN** as it does **not require any expensive computation**. It has been shown to **speed up the convergence of stochastic gradient descent algorithms**."],"metadata":{"id":"6vnTJ4xrShxc"}},{"cell_type":"markdown","source":["#Pooling layer\n","The **pooling layer is key to** ensuring that the subsequent layers of the CNN can **pick up larger-scale detail than just edges and curves**.\n","##Method 1: Max Pooling\n","Done by applying a **max filter** to non-overlapping subregions of\n","the initial representation.\n","\n","例子：\n","$$I = \\left(\\begin{matrix}\n","12 &20 &30 &0 \\\\\n","8 &12 &2 &0  \\\\\n","34 &70 &37 &4 \\\\\n","112 &100 &25 &12 \\end{matrix}\\right)$$\n","\n","After Pooling:\n","$$I = \\left(\\begin{matrix}\n","20 &30 \\\\\n","112 &37\n","\\end{matrix}\\right)$$\n","\n","##Method 2: Average Pooling\n","Done by applying a average filter to non-overlapping subregions of the initial representation.\n","\n","例子：\n","$$I = \\left(\\begin{matrix}\n","12 &20 &30 &0 \\\\\n","8 &12 &2 &0  \\\\\n","34 &70 &37 &4 \\\\\n","112 &100 &25 &12 \\end{matrix}\\right)$$\n","\n","After Pooling:\n","$$I = \\left(\\begin{matrix}\n","13 &16 \\\\\n","79 &19.5\n","\\end{matrix}\\right)$$"],"metadata":{"id":"F2PBxVcYT7Mb"}},{"cell_type":"markdown","source":["#Common Problems\n","##Underfitting\n","Underfitting typically refers to a model that **has not been trained sufficiently**. This could be due to insufficient training time or a model that was simply not trained properly. A model that is under fitted will perform poorly on the training data and new, unseen data alike.\n","\n","##Overfitting\n","Overfitting refers to a model that was **trained too much on the particulars of the training data** (when the model learns the noise in the dataset). A model that is overfitted will not perform well on new, unseen data.\n","\n","##Dropout layer:\n","**Dropout is a simple way to prevent neural networks from overfitting.**\n","\n","Dropout is a regularization method. During training, **some number of layer outputs are randomly ignored** or “dropped out”.\n","\n","注意：\n","\n","我们一般不会dropout input和output，一般只在Dense layer之间加入Dropout layer.\n","\n","A **common value is a probability of 0.5** for retraining the output of each node in a hidden layer."],"metadata":{"id":"LGS0tAM9z-7x"}},{"cell_type":"markdown","source":["##Procedures\n"," 1. Import required libraries and define global variables\n"," 2. Load and prepare the data\n"," 3. Build the model\n"," 4. Compile the model\n"," 5. Train the model\n"," 6. Evaluate the model accuracy\n"," 7. Save the model\n"," 8. Use the model"],"metadata":{"id":"59dJvIoN2HxV"}},{"cell_type":"markdown","source":["##One-Hot Encoding\n","to categorical() **converts class vector (integers from 0 to number of classes) to binary class matrix**, for use of categorical crossentropy."],"metadata":{"id":"cOZgWhWA3jh0"}},{"cell_type":"code","source":["import numpy as np\n","from keras.utils import to_categorical\n","\n","y_train = [1, 0, 3, 4, 5, 0, 2, 1]\n","y_one_hot = to_categorical(y_train, num_classes=6)\n","print(y_one_hot)\n","# Output:"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUCU7ixT3yDc","executionInfo":{"status":"ok","timestamp":1763434380212,"user_tz":-480,"elapsed":15,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"7fa0455d-ea8e-4797-f076-9305d5c92a36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 1.]\n"," [1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["为什么要使用One-Hot Encoding?\n","\n","这样可以让我们的training data不受index大小的ranking影响，比如说我们有data: [1, 2, 3], 它们只是分为了三个categories，但是它们的大小可能会影响我们的结果，所以我们把它变成One-Hot matrix，这样每个categories都是一样的：[[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n","\n","Benefits:\n","\n"," Training data is more usable and expressive as a result of one-hot encoding, and it can be re-scaled easily."],"metadata":{"id":"L11Rm3tY4kd3"}},{"cell_type":"markdown","source":["##Number of Parameters\n"],"metadata":{"id":"9J8HWyNvDTpA"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import datetime\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.layers import Dense, Dropout, Flatten # Import Dense, Dropout, Flatten class\n","from keras.utils import to_categorical\n","\n","# Import numpy-related utilities\n","from keras.callbacks import TensorBoard\n","from keras.models import load_model\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.math import confusion_matrix\n","from tensorflow.keras.metrics import categorical_crossentropy\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 5\n","img_rows, img_cols = 28, 28 # Image dimensions"],"metadata":{"id":"opvbgqWH7NMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.reshape(60000,28,28,1)  # Reshape the data to 4-dimension\n","x_test = x_test.reshape(10000,28,28,1)    # Reshape the data to 4-dimension\n","\n","y_train = to_categorical(y_train, num_classes)\n","y_test = to_categorical(y_test, num_classes)\n","\n","model = Sequential()\n","\n","model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n","\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n"," # Add a dropout layer to prevent a model from overfitting\n","model.add(Dropout(0.25))\n","\n"," # Add a flatten layer to convert the pooled data to a single column\n"," # that is passed to the fully-connected layer\n","model.add(Flatten())\n","model.add(Dense(units=64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(units=num_classes, activation='softmax'))\n","\n","model.compile(optimizer='adam', # Default learning rate is 0.001\n","              loss=categorical_crossentropy,\n","              metrics=['accuracy']\n","              )\n"],"metadata":{"id":"zJt-MpU27fi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_dir=\".logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","training_history = model.fit(x_train, y_train,\n"," batch_size=batch_size,\n"," epochs=epochs,\n"," validation_data=(x_test, y_test),\n"," callbacks=[tensorboard_callback]\n"," )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGwkwRMu8qaZ","executionInfo":{"status":"ok","timestamp":1763436731169,"user_tz":-480,"elapsed":123553,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"408df0aa-bf5a-4c4b-da01-daa2dccbc556"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - accuracy: 0.2277 - loss: 6.7428 - val_accuracy: 0.9014 - val_loss: 0.4482\n","Epoch 2/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 50ms/step - accuracy: 0.7190 - loss: 0.8579 - val_accuracy: 0.9600 - val_loss: 0.1456\n","Epoch 3/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.8671 - loss: 0.4239 - val_accuracy: 0.9729 - val_loss: 0.0983\n","Epoch 4/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 52ms/step - accuracy: 0.9080 - loss: 0.3024 - val_accuracy: 0.9758 - val_loss: 0.0870\n","Epoch 5/5\n","\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 53ms/step - accuracy: 0.9250 - loss: 0.2490 - val_accuracy: 0.9791 - val_loss: 0.0709\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"id":"TSCBhBtW9ken","executionInfo":{"status":"ok","timestamp":1763436738156,"user_tz":-480,"elapsed":32,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"48528542-b624-48ef-f759-e8b4c04cbd2a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_5\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m51,264\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m170,144\u001b[0m (664.63 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170,144</span> (664.63 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,714\u001b[0m (221.54 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,714</span> (221.54 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m113,430\u001b[0m (443.09 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,430</span> (443.09 KB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["validation_loss, validation_accuracy = model.evaluate(x_test, y_test, verbose=0)\n","# Print loss and accuracy\n","print('Validation loss:', validation_loss)\n","print('Validation accuracy:', validation_accuracy)"],"metadata":{"id":"UMeajPgz8zPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Typical CNN Architecture\n","1. few convolutional layers (usually with ReLU activation)\n","2. a pooling layer\n","3. a few convolutional layers (usually with ReLU activation) and a pooling layer, and so on\n","4. Then, one or more fully connected layers (usually with ReLU activation)\n","5. Then, a final fully connected softmax layer\n","\n","Generally, the **convolutional and pooling layers are for feature learning**, and the **fully-connected layers are for classification**.\n","\n","Instead of using a convolutional layer with a large kernel size (e.g. 9×9 with 81 parameters), it is better to **stack two convolutional layers with smaller kernels** (e.g. 3×3) with 18 parameters for the two layers together."],"metadata":{"id":"x2umNygUBcj1"}},{"cell_type":"code","source":["# implement Convolution from Scratch\n","import numpy as np\n","def convolve(image, kernel):\n","  img_h, img_w = image.shape\n","  ker_h, ker_w = kernel.shape\n","  # flip the kernel\n","  kernel = kernel[::-1, ::-1]\n","  new_h = img_h - ker_h + 1\n","  new_w = img_w - ker_w + 1\n","  new_img = np.zeros((new_h, new_w))\n","  for i in range(0, new_h):\n","    for j in range(0, new_w):\n","      new_img[i, j] = np.sum(image[i : i + ker_h, j : j + ker_w] * kernel)\n","\n","  return new_img"],"metadata":{"id":"nvV7wv5fA2Kh","executionInfo":{"status":"ok","timestamp":1765416009460,"user_tz":-480,"elapsed":6,"user":{"displayName":"王知新","userId":"09870380652686636069"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","if __name__ == '__main__':\n","  kernel = np.array([[-1, 3, 1],\n","                     [7, 2, 2],\n","                     [-3, -2, 1]])\n","  text =\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 55 136 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 172 253 0 0 0 0 0 0 0 0 0 0 \\\n","   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18 226 253 0 0 0 0 0 0 0 0 0 0 \\\n","   49 18 0 0 0 0 0 0 0 0 0 0 0 0 0 171 253 253 0 0 0 0 0 0 0 0 0 \\\n","   30 238 219 80 0 0 0 0 0 0 0 0 0 0 0 23 219 253 212 0 0 0 0 0 0 \\\n","   0 0 0 36 253 253 156 14 0 0 0 0 0 0 0 0 0 0 66 253 253 135 0 0 \\\n","   0 0 0 0 0 0 0 94 253 253 107 1 0 0 0 0 0 0 0 0 0 24 213 253 253 \\\n","   132 0 0 0 0 0 0 0 0 0 154 253 253 253 154 139 11 0 0 0 0 0 0 0 \\\n","   114 253 253 244 16 0 0 0 0 0 0 0 0 3 170 253 253 253 253 253 190 \\\n","   35 0 0 0 0 0 39 221 253 253 133 0 0 0 0 0 0 0 0 0 18 253 253 253 \\\n","   205 90 190 253 241 81 0 0 0 0 148 253 253 195 11 0 0 0 0 0 0 0 0 \\\n","   0 18 253 253 198 11 0 2 70 225 240 45 0 0 46 229 253 253 80 0 0 0 \\\n","  0 0 0 0 0 0 0 18 253 253 182 0 0 0 0 160 253 186 16 0 130 253 253 \\\n","  198 9 0 0 0 0 0 0 0 0 0 0 126 253 253 247 43 0 0 0 108 253 253 93 \\\n","  0 183 253 253 81 0 0 0 0 0 0 0 0 0 0 0 136 253 251 241 154 0 0 0 1 \\\n","  119 253 252 249 253 253 201 2 0 0 0 0 0 0 0 0 0 0 0 175 225 93 0 0 \\\n","  0 0 0 0 25 150 253 253 253 250 78 0 0 0 0 0 0 0 0 0 0 0 0 26 172 82 \\\n","  0 0 0 0 0 0 0 27 187 249 207 182 0 0 0 0 0 0 0 0 0 0 0 0 0 166 253 82 \\\n","  0 0 0 0 0 0 0 0 0 64 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 255 242 56 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 247 195 39 0 0 0 0 0 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\\n","  0 0 0 0 0 0 0 0\"\n","  text = text.split()\n","  image = np.array(text).astype(np.int64).reshape(28, 28)\n","  plt.subplot(1, 2, 1)\n","  plt.imshow(image, 'gray')\n","  plt.title(\"before convolve:\")\n","  plt.subplot(1, 2, 2)\n","  new_image = convolve(image, kernel)\n","  plt.imshow(new_image, 'gray')\n","  plt.title('after convolve:')\n","  plt.show()\n"],"metadata":{"id":"0VKxeznZGeOZ","colab":{"base_uri":"https://localhost:8080/","height":308},"executionInfo":{"status":"ok","timestamp":1765417294647,"user_tz":-480,"elapsed":303,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"20d5a5fd-f47f-456e-e2f0-6ac2a5bdcde7"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]}]}