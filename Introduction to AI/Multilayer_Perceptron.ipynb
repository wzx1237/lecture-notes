{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOscikD+6Vad7B0A5kCmpY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["10 月 21 号\n","#ANN Multilayer Perceptron\n","\n","Recall the artificial neuron is a simple biological neuron model in an artificial neural network.\n","\n","It has a couple of limitations:\n","1. Can only represent a limited set of functions.\n","2. Can only distinguish (by the value of its output) the sets of inputs that are linearly separable in the inputs.\n","\n","即使我们使用了non-linear activation function, single artificial neuron 也只能进行binary classification\n","\n","for example, let's consider: $f(x) = \\frac{1}{1 + e^{-x}}$, and classify $f(x) > 0.5$ as one category. We find that: when $x > 0$, $f(x) > 0.5$. So, our decision boundary is also linear.\n","($x = \\sum\\omega_{i} x_{i}$)\n","\n"],"metadata":{"id":"nhpKtvBzm3z8"}},{"cell_type":"markdown","source":["#Multi-Layer Perceptron Neural Network\n","\n","Inorder ro solve the problem, we introduce the Multi-layer perceptron (MLP) neural network.\n","\n","Multi-layer perceptron (MLP) neural network is a type of feed-forward neural network.\n","\n","It consists of three types of layers:\n"," 1. Input layer (also called layer i)\n"," 2. Hidden layer (also called layer j)\n"," 3. Output layer (also called layer k)\n","\n","\\begin{matrix}\n","  &input-layer   &hidden-layer    &output-layer \\\\\n","  &x_{1}         &\\sum \\quad f    &\\sum \\quad g  \\\\\n","  &x_{2}         &\\sum \\quad f    &\\sum \\quad g  \\\\\n","  &x_{3}         &\\sum \\quad f    &\\sum \\quad g\n","\\end{matrix}\n","\n","**Note:**\n","here, $f$ and $g$ can be different. In fact, we would like to choose different $f$ and $g$ in real practice, because this can give us different gradient, which may give our better fit."],"metadata":{"id":"Hn8xPVbzpHjt"}},{"cell_type":"markdown","source":["##How to initialize the weights and biases?\n","Answer: Initialize them to some small random values."],"metadata":{"id":"VWQy6BpKrHMm"}},{"cell_type":"markdown","source":["## How to perform training?\n","\n","Answer:\n"," 1. Let the network calculate the output with the given inputs (forward propagation)\n"," 2. Calculate the error/loss function (i.e. the difference between the calculated outputs and the target outputs)\n"," 3. Update the weights and biases between the hidden and output layer (backward propagation)\n"," 4. Update the weights and biases between the input and hidden layer (backward propagation)\n"," 5. Go back to step 1\n","\n"," ## When to stop training?\n","\n"," Answer:\n"," 1. After a fixed number of iterations through the loop.\n"," 2. Once the training error falls below some threshold.\n"," 3. Stop at a minimum of the error on the validation set."],"metadata":{"id":"SOpyFFbmrOaA"}},{"cell_type":"markdown","source":["##How to update the weights and biases?\n","Assuming the activation function is the sigmoid function: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. The error function is: $E = \\frac{1}{2} \\sum (O_{i}-T_{i})^{2}$\n","\n","The derivative of sigmoid function satisfy: $\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1-\\sigma(x))$\n","\n","consider:\n","\\begin{align}\n","O_{k} &= \\sigma(\\theta_{k} + \\sum \\omega_{i}O_{j}) = \\sigma(x), \\\\\n","\\frac{\\partial O_{k}}{\\partial \\theta_{k}} &= \\frac{\\partial O_{k}}{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{j})} \\cdot \\frac{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{j})}{\\partial \\theta_{k}} \\\\\n","&= \\frac{\\partial \\sigma(x)}{\\partial x} \\cdot 1 \\\\\n","&= \\sigma(x)(1-\\sigma(x)) \\\\\n","&= O_{k}(1-O_{k})\n","\\end{align}\n","\n","So, we can obtain the gradient of $\\theta_{k}$:\n","\\begin{align}\n","\\frac{\\partial E}{\\partial \\theta_{k}} &= \\frac{\\partial (\\frac{1}{2} \\sum (O_{i}-T_{i})^{2})}{\\partial O_{k}} \\cdot \\frac{\\partial O_{k}}{\\partial \\theta_{i}} \\\\\n","&= (O_{k} - T_{k}) \\cdot \\frac{\\partial O_{k}}{\\partial \\theta_{i}} \\\\\n","&= (O_{k} - T_{k})O_{k}(1-O_{k})\n","\\end{align}\n","\n","So, by the same way, the gradient of $\\theta_{j}$:\n","\\begin{align}\n","O_{j} &= \\sigma(\\theta_{j} + \\sum \\omega_{i}O_{i}) \\\\\n","\\frac{\\partial E}{\\partial \\theta_{j}}\n","&= \\frac{\\partial E}{\\partial O_{j}} \\cdot \\frac{\\partial O_{j}}{\\partial \\theta_{j}} \\\\\n","&= (\\sum_{k} \\frac{\\partial E}{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{i})} \\cdot \\frac{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{i})}{\\partial O_{j}}) \\cdot \\frac{\\partial O_{j}}{\\partial \\theta_{j}} \\\\\n","&= (\\sum_{k} \\delta_{k}\\cdot\\omega_{jk}) \\cdot \\frac{\\partial O_{j}}{\\partial \\theta_{j}} \\\\\n","&= O_{j}(1-O_{j})\\sum_{k} \\delta_{k}\\cdot\\omega_{jk}\n","\\end{align}\n","\n","where, $\\delta_{k} = (O_{k} - T_{k})O_{k}(1-O_{k})$, which is the gradient of $\\theta_{k}$\n","\n","**Note:**\n","Here, the second equation is because: $O_{j}$ would influence all output layer nodes. So, when we take the gradient, we need to consider all: $\\theta_{k} + \\sum \\omega_{i}O_{i}$ , and sum them up.\n","\n","同样的，我们可以得到$\\omega_{j}$和$\\omega_{k}$的gradient:\n","\n","\\begin{align}\n","\\frac{\\partial O_{k}}{\\partial \\omega_{k}} &= \\frac{\\partial O_{k}}{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{j})} \\cdot \\frac{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{j})}{\\partial \\omega_{k}} \\\\\n","&= \\frac{\\partial \\sigma(x)}{\\partial x} \\cdot O_{j} \\\\\n","&= \\sigma(x)(1-\\sigma(x))O_{j} \\\\\n","&= O_{k}(1-O_{k})O_{j}\n","\\end{align}\n","\n","**AND:**\n","\\begin{align}\n","\\frac{\\partial E}{\\partial \\omega_{j}}\n","&= \\frac{\\partial E}{\\partial O_{j}} \\cdot \\frac{\\partial O_{j}}{\\partial \\omega_{j}} \\\\\n","&= (\\sum_{k} \\frac{\\partial E}{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{i})} \\cdot \\frac{\\partial (\\theta_{k} + \\sum \\omega_{i}O_{i})}{\\partial O_{j}}) \\cdot \\frac{\\partial O_{j}}{\\partial \\omega_{j}} \\\\\n","&= (\\sum_{k} \\delta_{k}\\cdot\\omega_{jk}) \\cdot \\frac{\\partial O_{j}}{\\partial \\omega_{j}} \\\\\n","&= O_{j}(1-O_{j})O_{i}\\sum_{k} \\delta_{k}\\cdot\\omega_{jk}\n","\\end{align}\n","\n","**where:**\n","\n"," $O_{j}$ is the output of layer $j$ , $O_{i}$ is the output of layer $i$ , and $O_{k}$ is the output of layer $k$.\n","\n","$\\theta_{j}$ is the bias of layer $j$ , and $\\theta_{k}$ is the bias of layer $k$.\n","\n","$\\omega_{j}$ is the weight of layer $j$ , and $\\omega_{k}$ is the weight of layer $k$. Specially,  $w_{ij}$ is the weight connecting node in layer $i$ to node in layer $j$ ; $w_{jk}$ is the weight connecting node in layer $j$ to node in layer $k$."],"metadata":{"id":"vJ94v8QPsZZF"}},{"cell_type":"markdown","source":["**From above discussion, we obtain the following formula for updating weights and biases:**\n","\n","**denote:**\n","\n","\\begin{align}\n","\\delta_{k} &= O_{k}(1-O_{k}) \\\\\n","\\delta_{j} &= O_{j}(1-O_{j})\\sum_{k \\in K} \\delta_{k}\\cdot\\omega_{jk}\n","\\end{align}\n","\n","**Updating rule:**\n","\\begin{align}\n","w_{jk} &= wjk −ηδ_kO_j \\\\\n"," w_{ij} &= wij −ηδ_jO_i \\\\\n"," θ_k &= θ_k −ηδ_k \\\\\n"," θ_j &= θ_j −ηδ_j\n","\\end{align}\n","\n","where, $\\eta$ is learning rate.\n","\n"],"metadata":{"id":"W4V2f2O808gL"}},{"cell_type":"code","source":["import numpy as np # Import NumPy\n","\n","class MultiLayerPerceptron:\n","  def __init__(self):\n","    \"\"\" Multi-layer perceptron initialization \"\"\"\n","    self.wij = np.array([    # Weights between input and hidden layer\n","      [-0.65, 0.64],         # w1, w2\n","      [1.11, 0.84]           # w3, w4\n","    ])\n","    self.wjk = np.array([      # Weights between hidden and output layer\n","      [0.86],                  # w5\n","      [-1.38]                  # w6\n","    ])\n","    self.tj = np.array([        # Biases of nodes in the hidden layer\n","      [0.0],                    # theta 1\n","      [0.0]                     # theta 2\n","    ])\n","    self.tk = np.array([[0.0]]) # Bias in the output layer, Theta 3\n","    self.learning_rate = 0.5    # Eta\n","    self.max_round = 10000      # Number of rounds\n","\n","  def sigmoid(self, z, sig_cal=False):\n","    \"\"\" Sigmoid function and the calculation of z * (1-z) \"\"\"\n","    # If sig_cal is True, return sigmoid\n","    if sig_cal: return 1 / (1 + np.exp(-z))\n","\n","    # If sig_cal is False, return z * (1-z)\n","    return z * (1-z)\n","\n","  def forward(self, x, predict=False):\n","    \"\"\" Forward propagation \"\"\"\n","    # Get the training example as a column vector. Shape (2,1)\n","    sample = x.reshape(len(x), 1)\n","\n","    # Compute the hidden node outputs. Shape (2,1)\n","    # w1  w2        x1        w1*x1 + w2*x2\n","    # w3  w4    @   x2   =    w3*x1 + w4*x2\n","    yj = self.sigmoid(self.wij.dot(sample) + self.tj, sig_cal=True)\n","\n","    # Compute the output of node in the output layer. Shape (1,1)\n","    yk = self.sigmoid(self.wjk.transpose().dot(yj) + self.tk, sig_cal=True)\n","\n","    # If predict is True, return the output of node in the layer node\n","    if predict: return yk\n","\n","    # Return (data sample, hidden node outputs, predicted output)\n","    return (sample, yj, yk)\n","\n","  def backpropagation(self, values, Tk):\n","    # values: return by forward\n","    # Tk: target\n","\n","    Oi = values[0] # Input sample\n","    Oj = values[1] # Hidden node outputs\n","    Ok = values[2] # Predicted output\n","\n","    \"\"\" back propagation \"\"\"\n","    # deltak = (Ok-Tk)Ok(1-Ok). Shape (1,1)\n","    # here, sigmoid would return the derivative by default\n","    deltaK = np.multiply((Ok- Tk), self.sigmoid(Ok))\n","\n","    # deltaj = Oj(1-Oj)(deltak)(Wjk). Shape (2,1)\n","    #\n","    deltaJ = np.multiply(self.sigmoid(Oj), deltaK[0][0] * self.wjk)\n","\n","    \"\"\" start update weight and bias \"\"\"\n","    # wjk = wjk- eta(deltak)(Oj). Shape (2,1)\n","    self.wjk-= self.learning_rate * deltaK[0][0] * Oj\n","\n","    # thetak = thetak- eta(deltak). Shape (1,1)\n","    self.tk-= self.learning_rate * deltaK\n","\n","    # wij = wij- eta(deltaj)(Oi). Shape (2,2)\n","    # delta1                    delta1*x1     delta1*x2\n","    # delta2    @   x1  x2  =   delta2*x1     delta2*x2\n","    s = self.learning_rate * deltaJ.dot(Oi.T)\n","\n","    # w1  w2        delta1*x1     delta1*x2\n","    # w3  w4    -   delta2*x1     delta2*x2\n","    self.wij-= s\n","\n","    # thetaj = thetaj- eta(deltaj). Shape (2,1)\n","    self.tj-= self.learning_rate * deltaJ\n","\n","  def train(self, X, T):\n","    \"\"\" Training \"\"\"\n","    for i in range(self.max_round): # Train max round number of rounds\n","      for j in range(m):\n","        # Use all the samples\n","        # print(f'Iteration: {i+1} and {j+1}')\n","        values = self.forward(X[j])        # Forward propagation\n","        self.backpropagation(values, T[j]) # Back propagation\n","\n","  def print(self):\n","    print(f'wij: {self.wij}')\n","    print(f'wjk: {self.wjk}')\n","    print(f'tj: {self.tj}')\n","    print(f'tk: {self.tk}')\n"],"metadata":{"id":"uKk4RWBe7v4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  m = 4\n","  X = np.array([ # Input data\n","      [0, 0],\n","      [0, 1],\n","      [1, 0],\n","      [1, 1]\n","  ])\n","  T = np.array([ # Target values\n","      [0],\n","      [1],\n","      [1],\n","      [0]\n","  ])\n","  mlp = MultiLayerPerceptron() # Create an object\n","  mlp.train(X, T)\n","  mlp.print()\n","  for k in range(m):\n","      Ok = mlp.forward(X[k],True)   # forward propagation\n","      print(f'y{k}: {Ok}')          # backward propagation"],"metadata":{"id":"98F0YWwZAnAA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761189091057,"user_tz":-480,"elapsed":1670,"user":{"displayName":"王知新","userId":"09870380652686636069"}},"outputId":"ed486692-decd-4aaf-ee73-b5463c2294c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wij: [[-6.25165367  6.22962318]\n"," [-5.7179006   5.96706892]]\n","wjk: [[ 9.53377675]\n"," [-9.2900534 ]]\n","tj: [[-3.40695349]\n"," [ 2.8512169 ]]\n","tk: [[4.41777385]]\n","y0: [[0.01697257]]\n","y1: [[0.98413923]]\n","y2: [[0.98051328]]\n","y3: [[0.0151785]]\n"]}]},{"cell_type":"markdown","source":["#Handwritten Digits Recognition using MLP\n","\n"," We will build a MLP Artificial Neural Network to recognize/classify handwritten digits.\n","\n"," **Terminologies:**\n"," 1. **Training data:**\n"," The data our model learn from. Sometimes split into training and validation\n"," data.\n"," 2. **Testing data:**\n"," The data is kept secret from the model until after it has been trained. Testing data is used to evaluate our model.\n"," 3. **Loss function:**\n","  A function used to\n","  *do differentiation to update parameters* during training,\n","  monitor each epoch to *decide the convergence* during training, *quantify how accurate a model’s predictions were.*\n","  **The only objective of the neural network is optimizing/minimizing the loss function.**\n"," 4. **Optimization algorithm:**\n"," It controls exactly **how the parameters are adjusted during training**. (E.g., gradient descent.)\n","\n","\n","##Dataset\n","We use the Modified National Institute of Standards and Technology\n","(MNIST) dataset.\n","\n","This dataset contains two sets of samples:\n"," 1. Training data: 60000 28 pixel × 28 pixel images of handwritten digits from 0 to 9.\n"," 2. Testing data: 10000 28 pixel × 28 pixel images.\n"],"metadata":{"id":"A5caVhRuE9AY"}},{"cell_type":"markdown","source":["#Procedures\n","\n"," 1. Import the required libraries and define a global variable\n"," 2. Load the data\n"," 3. Explore the data\n"," 4. Build the model\n"," 5. Compile the model\n"," 6. Train the model\n"," 7. Evaluate the model accuracy\n"," 8. Save the model\n"," 9. Use the model\n"," 10. Plotting the confusion matrix"],"metadata":{"id":"PanzBn8wHGxb"}},{"cell_type":"markdown","source":["##Build the Model\n","\n","**how to get the number of parameter between hidden layer?**\n","\\begin{align}\n","parameter = (prev + 1) \\times current\n","\\end{align}\n","where $parameter$ means trainable parameter number, $prev$ means number of node in previous layer, $current$ means number of node in current layer.\n","\n","**Remark:**\n","\n","Usually, we would multiply a small factor in hidden layer when we are updating parameters. (E.x. $\\omega_{ij} = \\omega_{ij} + (\\eta \\alpha \\nabla E) \\cdot x_{ij}$, where $\\alpha$ is the small number ($\\alpha = 1$), $\\eta$ is the learning rate, and $\\nabla E$ is the gradient.)\n","\n","This is because the **gradient would increase a lot during back-propagation**. So, we need multiply a small number ($\\alpha$) to **avoid overfitting**."],"metadata":{"id":"8pkP1pndC20m"}},{"cell_type":"markdown","source":["##Optimization\n","\n","**Categorical/Multi-class cross-entropy loss:**\n","\\begin{align}\n","CCE = - \\sum_{i \\in C} y_{i} \\cdot log(p_{i})\n","\\end{align}\n","where $y$ represents the actual labels, usually the one-hot vector; $p$ represents predictions, usually the output of Softmax.\n","\n","**Usage:**\n","1. Mostly used to measure the **distance between two distributions**.\n","2. Measures **how close the predictions are to the actual labels**."],"metadata":{"id":"yj6GSQizEVCj"}},{"cell_type":"markdown","source":["##Save the Model\n"," Save the entire model to an HDFS (Hadoop Distributed File System) file.\n","\n"," The .h5 extension of the file indicates that the model should be **saved in Keras format as an HDFS file**."],"metadata":{"id":"un5jHEnAJa90"}},{"cell_type":"code","source":["model_name = 'digits_recognition_mlp.h5'\n","model.save(model_name, save_format='h5')\n","loaded_model = load_model(model_name)"],"metadata":{"id":"DfvjDPRnJnzv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Analysis on MLP\n","\n","##Problem: Vanishing Gradient and Exploding Gradient\n","**Vanishing Gradient:** the gradient is very small, updating is very slow.\n","\n","**Exploding Gradient:** the updated weight becomes NaN\n","\n","**For vanishing gradient:**\n","1. The parameters of the higher layers vary dramatically, whereas the parameters of the lower levels do not change significantly for vanishing (or not at all).\n","2. During training, the model weights may become zero.\n","3. The model learns slowly, and after a few cycles, the training may become stagnant.\n","\n","**For exploding gradient:**\n","1. The model parameters are growing exponentially.\n","2. During training, the model weights may become NaN.\n","3. The model goes through an avalanche learning process."],"metadata":{"id":"X6z4bNEZLo5n"}},{"cell_type":"markdown","source":["## Problem: Overfitting and Underfitting\n","**Overfitting:**\n"," It refers to a model that models the training data too well. It happens when a model **learns the detail and noise in the training data** to the extent that it negatively impacts the performance of the model on the new data.\n","\n","**Underfitting:**\n"," It refers to a model that can neither model the training data nor generalize to new data.\n","\n","**Avoid Overfitting是MLP的一个重要研究方向**"],"metadata":{"id":"g5ZIkoJSMttO"}},{"cell_type":"markdown","source":["##How Many Layers and Number of Neurons in Each of These Layers?\n","\n","**经验公式：**\n","\n","**Number of layers:**\n","1. If our data is linearly separable, NO hidden layer at all.\n","2. If data is less complex and has few dimensions or features,\n","   neural networks with 1 to 2 hidden layers would work.\n","3. If data has large dimensions or features, 3 to 5 hidden layers\n","   can be used to get an optimum solution.\n","\n","**Number of neurons:**\n","1. The number of hidden neurons should be between the size of the input layer and the output layer.\n","2. The most appropriate number of hidden neurons is:\n","\\begin{align}\n"," \\sqrt{input × output}\n","\\end{align}\n","3. The number of hidden neurons should keep decreasing in subsequent layers to get closer to pattern and feature extraction and identify the target class."],"metadata":{"id":"C1mjlZSTOE31"}},{"cell_type":"markdown","source":["##The effect of weights and bias\n","\n","Suppose we have the following perceptron:\n","\\begin{align}\n"," x \\rightarrow {Node} \\rightarrow f(\\omega x + \\theta)\n","\\end{align}\n","\n","Let’s get the output functions by setting w to 0.5, 1, 2, 3, θ to 0, and using sigmoid activation function.\n","\n","According to the example, we can see that **weights control the steepness of the activation function**.\n","\n","Now, let’s get another set of output functions by setting w to 1, θ to 0, 1, 2, and 3, and using the sigmoid activation function.\n","\n","According to the example, we can see that bias is used for shifting the activation function towards the left or right."],"metadata":{"id":"U-9J5jHYvh4A"}},{"cell_type":"markdown","source":["#Remark:\n","\n","**Build the Model**\n","\n","Layers\n","\n"," Layer 1: Flatten layer that will flatten 2D image into 1D\n","\n"," Layer 2: Hidden Dense layer 1 with 128 neurons and ReLU activation\n","\n"," Layer 3: Hidden Dense layer 2 with 128 neurons and ReLU activation\n","\n"," Layer 4: Output Dense layer with 10 Softmax outputs.\n","\n","##ReLU Activation Function:\n","$f(x) = max (0, x)$\n","\n","A derivative function and allows for backpropagation while simultaneously making it computationally efficient.\n","\n","The neurons will only be deactivated if the output of the linear transformation is less than 0.\n","\n","Accelerates the convergence of gradient descent due to its linear property.\n","\n","Limitations: **Dying ReLU problem**\n","\n","**The Dying ReLU Problem**\n","\n","The negative side of the graph makes the gradient value zero. During the backpropagation process, the weights and biases for some neurons are not updated.\n","\n","Dead neurons which never get activated. All the negative input values become zero immediately, which decreases the model’s ability to fit or train from the data properly.\n","\n","##Softmax Function:\n"," $f(z_{i}) = \\frac{e^{z_{i}}}{\\sum_{j} e^{z_{j}}}$\n","\n","Obviously, $\\sum f(z_{i}) = 1$, is a probability distribution\n","\n","**description:**\n","\n","1. A combination of multiple sigmoid\n","2. Calculates the relative probabilities. Similar to the sigmoid/\n","   logistic activation function, the Softmax function **returns the probability of each class**.\n","3. Most commonly used as an **activation function for the output\n","   layer of the neural network** in the case of multi-class classification.\n","\n","##Sigmoid Function:\n","$f(x) = \\frac{1}{1 + e^{-x}}$\n","\n","**Limitation:** The output of the sigmoid function is not symmetric around zero. So the output of all the neurons will be of the **same sign**. This makes the training of the neural network more difficult and unstable.\n","\n","##Tanh Activation Function:\n","$f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n","Very similar to the sigmoid activation function, and even\n","has the same S-shape with the difference in output range\n","of-1 to 1.\n","\n","The output of the tanh activation function is **Zero\n","centered**; hence we can easily map the output values as\n","strongly negative, neutral, or strongly positive.\n","\n","When used in hidden layers, the mean for the hidden\n","layer comes out to be 0 or very close to it. It helps in\n","centering the data and makes learning for the next layer\n","much easier.\n","\n","##Why Sigmoid and Tanh More Susceptible to Vanishing Gradients?\n","\n","The gradient values are only significant for range -3 to 3, and the graph gets much flatter in other regions.\n","\n","**For values greater than 3 or less than -3, the function will have very small gradients**. As the gradient value approaches zero, the network ceases to learn and suffers from the Vanishing gradients."],"metadata":{"id":"P5HGJ3G4LCIz"}},{"cell_type":"markdown","source":["#How to choose a hidden layer Activation function?\n","\n","| Neural Network | Commonly Used Activation Function |\n","| :------------  | :----------------                 |\n","| Multi-layer Perceptron (MLP) | ReLU activation function |\n","| Convolutional Neural Network (CNN) | ReLU activation function |\n","| Recurrent Neural Network (RNN) | Tanh and/or Sigmoid activation function|\n","\n","#How to choose an Output Activation function?\n","There are three commonly used activation functions for use in the output layer.\n","1. Linear\n","2. Sigmoid/Logistic\n","3. Softmax\n","\n","If a problem is a **regression problem**, we should use a **linear activation function**.\n","\n","If a problem is a **classification problem**, then there are three main types of classification problems, and each may use a different activation function.\n","1. **Binary classification:** One node, **sigmoid activation**.\n","2. **Multiclass classification:** One node per class, **softmax activation**.\n","3. **Multilabel classification:** One node per class, **sigmoid activation**.\n","\n","**Note:** Multiclass classification makes the assumption that each sample is assigned to one and only one label. Multilabel classification assigns to each sample a set of target labels."],"metadata":{"id":"dSAI8esszlQE"}},{"cell_type":"markdown","source":["##Backward propagation\n","\n","To give a basic concept here, for example, we have the below network structure.\n","\n","Given that $a_i = f_i(w_{i-1} a_{i-1} + \\theta_{i-1})$ where $a_0 = x$, $w_i$ is the weight parameter and $f_i$ is the activation function of layer i in the network.\n","\n","Network: <br>\n","input $x$  -($w_1$)-> $a_1,f_1$ -($w_2$)-> $a_2, f_2$ -($w_3$)-> $a_3, f_3$ --> $y$ --> Loss function $L$\n","\n","Let's say we now want to update $w_2$ through backpropagation. Then we will need to compute $w_2 = w_2 - \\eta \\nabla w_2$\n","\n","$\\nabla w_2 = \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial f_3}\\frac{\\partial f_3}{\\partial a_3}\\frac{\\partial a_3}{\\partial f_2}\\frac{\\partial f_2}{\\partial a_2}\\frac{\\partial a_2}{\\partial w_2}$\n","\n","while $\\nabla w_3 = \\frac{\\partial L}{\\partial w_3} = \\frac{\\partial L}{\\partial f_3}\\frac{\\partial f_3}{\\partial a_3}\\frac{\\partial a_3}{\\partial w_3}$, and you will find that some computation results can be reused.\n","\n","The following provides a simple animation showing how gradient descent works on regressing a basic function $3x^2$."],"metadata":{"id":"SX3WjecFtQMF"}}]}