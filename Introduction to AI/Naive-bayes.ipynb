{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPL7PqbsoqY9FbvVSpbwD0E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","##naive-bayes\n","9æœˆ18å·\n","\n","ä¸‰ç§mechine learning\n","1. supervised learning\n","2. unsuperivsed learning\n","3. reinforcement learning\n","\n","è´å¶æ–¯å…¬å¼ä¸æ¡ä»¶æ¦‚ç‡\n","\n","P(B|A): è¡¨ç¤ºAæ¡ä»¶ä¸‹ï¼ŒBå‘ç”Ÿçš„æ¦‚ç‡\n","\n","æ¡ä»¶æ¦‚ç‡å…¬å¼ï¼š\n","\\begin{align}\n","  P(B|A) = \\frac{P(AB)}{P(A)}\n","\\end{align}\n","è´å¶æ–¯å…¬å¼:\n","\\begin{align}\n","  P(B|E) = \\frac{P(B)P(E|B)}{P(E)}\n","\\end{align}\n","where, $E$ is called **evidence**, $B$ is called **belief**,\n","$P(B|E)$ is called **Posterior probability**, $P(B)$ is called\n","**Prior probability**, $P(E|B)$ is called **likelihood**\n","\n","åˆå› ä¸º:\n","\\begin{align}\n","  P(E) = P(E|B)P(B) + P(E|\\neg B)P(\\neg B)\n","\\end{align}\n","æ•…ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè´å¶æ–¯å…¬å¼å†™æˆå¦‚ä¸‹å½¢å¼ï¼š\n","\\begin{align}\n","  P(B|E) = \\frac{P(B)P(E|B)}{P(E|B)P(B) + P(E|\\neg B)P(\\neg B)}\n","\\end{align}\n","åœ¨æ›´ä¸€èˆ¬çš„å½¢å¼ä¸‹ï¼š\n","\n","\\begin{align}\n","  P(B_{i}|E) = \\frac{P(B_{i})P(E|B_{i})}{P(E|B_{1})P(B_{1}) + P(E|B_{2})P(B_{2}) + ... + P(E|B_{n})P(B_{n}))}= \\frac{P(B_{i})P(E|B_{i})}{\\sum_{j=1}^{n} P(E|B_{j})P(B_{j})}\n","\\end{align}\n","\n","åœ¨å®é™…åº”ç”¨çš„æ—¶å€™ï¼Œæˆ‘ä»¬çš„Eå¯èƒ½æ˜¯pieces of evidence:\n","\\begin{align}\n","  E = (e_{1}, e_{2}, ..., e_{d})\n","\\end{align}\n","ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œæˆ‘ä»¬å‡è®¾è¿™äº›$e_{i}$ç›¸äº’ç‹¬ç«‹\n","\n","æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„è´å¶æ–¯å…¬å¼å°±å˜æˆäº†:\n","\\begin{align}\n","    P(B_{i}|E) = \\frac{P(B_{i})\\prod_{j=1}^{d} P(E_{j}|B_{i})}{\\sum_{k=1}^{n} [\\prod_{j=1}^{d} P(E_{j}|B_{k})]P(B_{k})}\n","\\end{align}\n","å¦‚æœæˆ‘ä»¬æƒ³çŸ¥é“åœ¨ä¼—å¤š$B_{i}$ä¸­ï¼Œå“ªä¸€ä¸ªå‘ç”Ÿçš„æ¦‚ç‡æœ€é«˜ï¼Œæˆ‘ä»¬åªéœ€è¦æ¯”è¾ƒåˆ†å­å³å¯,\n","å› ä¸ºåˆ†æ¯çš„å€¼éƒ½æ˜¯$P(E)$ï¼Œæ²¡æœ‰åŒºåˆ«\n","\\begin{align}\n","    B_{NB} = \\max {P(B_{i})\\prod_{j=1}^{d} P(E_{j}|B_{i})}\n","\\end{align}\n","\n","##Laplace smoothing\n","æœ‰äº›æ—¶å€™ï¼Œæˆ‘ä»¬å¯èƒ½åœ¨training dataé‡Œé¢æ²¡æœ‰è¿™ä¸ªæ•°æ®:\n","\\begin{align}\n","    P(E_{j}|B_{i}) = 0\n","\\end{align}\n","è¿™æ—¶å€™ä¼šé€ æˆä¸€äº›ç¾éš¾æ€§åæœï¼šæ‰€æœ‰çš„$P$éƒ½å˜æˆ0äº†...\n","\n","è¿™æ ·çš„è¯ï¼Œç›´æ¥ä½¿ç”¨è´å¶æ–¯å…¬å¼ä¼šå‡ºç°é—®é¢˜ï¼Œç§°ä½œï¼šzero frequency\n","\n","ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¦ä½¿ç”¨\\alpha-Laplace smooth\n","\n","å¼•å…¥ä¿®æ­£é¡¹ï¼š$\\alpha$å’Œ$m\\alpha$, where m is the number of \"$e_{j}$\"\n","\n","åœ¨è®¡ç®—ä¸€é¡¹çš„æ—¶å€™ï¼š\n","\\begin{align}\n","    P(e_{j}|B_{i}) = \\frac{count(e_{j}=v | B_{i}=b) + \\alpha}{count(B{i}=b) + m\\alpha}\n","\\end{align}\n","è¿™é‡Œï¼Œcount(A)è¡¨ç¤ºåœ¨sampleä¸­æ»¡è¶³æ‹¬å·ä¸­æ¡ä»¶Açš„sampleçš„ä¸ªæ•°\n","\n","å…³äºä¸ºä»€ä¹ˆLaplace smoothè¦ä¹˜ä»¥mï¼š\n","\n","This ensures that the total probability across all categories remains normalized.\n","By multiplying ($\\alpha$) by $m$, we proportionally distribute the smoothing effect across all categories.\n","Laplace smoothing ensures that the probabilities remain normalized, meaning they sum up to 1.\n","Let's go through the mathematical derivation and an example to illustrate this:\n","\n","Given:\n","\n","$N_i$: Original count of occurrences for category $i$.\n","\n","$m$: Total number of categories.\n","\n","$\\alpha$: Smoothing parameter.\n","The smoothed probability for category $i$ is calculated as:\n","\\begin{align}\n","    P(i) = \\frac{N_i + \\alpha}{N + m\\alpha}\n","\\end{align}\n","Where:\n","\n","$N$ is the total count of occurrences across all categories before smoothing.\n","Derivation\n","Numerator: \\begin{align}N_i + \\alpha\\end{align}\n","\n","We add $\\alpha$ to each category count to ensure no category has a zero probability.\n","Denominator: \\begin{align}N + m\\alpha\\end{align}\n","\n","Total count $N$ is increased by $m\\alpha$ to account for the $\\alpha$ added to each of the $m$ categories.\n","Example\n","Suppose you have a feature with 3 categories: A, B, and C. Their original counts are:\n","\\begin{align}\n","  N_A = 3 \\\\\n","  N_B = 2\\\\\n","  N_C = 0\\\\\n","  Total_N = N_A + N_B + N_C = 5\n","\\end{align}\n","Let's apply Laplace smoothing with $\\alpha = 1$.\n","\n","Smoothed Count:\n","\\begin{align}\n","    (N_A + \\alpha = 3 + 1 = 4)\n","    (N_B + \\alpha = 2 + 1 = 3)\n","    (N_C + \\alpha = 0 + 1 = 1)\n","\\end{align}\n","\n","Smoothed Total Count:\n","\\begin{align}\n","    (N + m\\alpha = 5 + 3 \\times 1 = 8)\n","\\end{align}\n","Smoothed Probabilities:\n","\\begin{align}\n","    P(A) = \\frac{4}{8} = 0.5 \\\\\n","    P(B) = \\frac{3}{8} = 0.375 \\\\\n","    P(C) = \\frac{1}{8} = 0.125\n","\\end{align}\n","Normalization Check\n","The probabilities should sum to 1:\n","\\begin{align}\n","    P(A) + P(B) + P(C) = 0.5 + 0.375 + 0.125 = 1\n","\\end{align}\n","Laplace smoothing ensures that the sum of probabilities across all categories is 1, maintaining a valid probability distribution."],"metadata":{"id":"qKimLY971WvN"}},{"cell_type":"markdown","source":["ä¸€èˆ¬æ¥è¯´ï¼Œåˆ›é€ AIéœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n","1. training\n","2. testing\n","\n","å¦‚ä½•è®¡ç®—è¿ç»­å‹å˜é‡çš„probability:\n","\\begin{align}\n","  f(x) = \\frac{1}{\\sqrt{2Ï€}Ïƒ}exp(-\\frac{(x-ğ)^2}{2Ïƒ^2})\n","\\end{align}"],"metadata":{"id":"kMbwGUVfwl4I"}},{"cell_type":"code","source":["import numpy as np\n","# CategoricalNB is for doing Naive Bayes classifier for categorical features\n","from sklearn.naive_bayes import CategoricalNB\n","# Forming training data\n","# Features: Blood Pressure (0=High,1=Normal,2=Low), Fever (0=High,1=Mild,2=No Fever),\n","#       Diabetes (0=Yes,1=No), Vomit (0=Yes,1=No)\n","training = np.array([[0, 0, 0, 1], [0, 0, 0, 0], [2, 0, 0, 1], [1, 1, 0, 1], [1, 2, 1, 1],\n","          [1, 2, 1, 0], [2, 2, 1, 0], [0, 1, 0, 1], [0, 2, 1, 1], [1, 1, 1, 1],\n","          [0, 1, 1, 0], [2, 1, 0, 0], [2, 0, 1, 1], [1, 1, 0, 0]])\n","# Forming the label set\n","# Labels: 0=Yes, 1=No\n","outcome = np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1])\n","# This is the new data to be evaluated using the trained AI (model)\n","new_sample = np.array([[0, 2, 0, 0]])\n","# Train Naive Bayes classifier according to training, outcome\n","# Î±éå¸¸çš„å°ï¼Œå› ä¸ºæˆ‘ä»¬çš„training data setå¾ˆå°ã€‚æˆ‘ä»¬ä¸æƒ³è®©Î±å½±å“ç»“æœå¤ªå¤š\n","clf = CategoricalNB(alpha=1.0e-10).fit(training, outcome) # fitä»£è¡¨training\n","# Perform classification on new_sample, and get the probability estimates\n","pred_class = clf.predict(new_sample)  # classification result\n","prob = clf.predict_proba(new_sample)  # probability\n","# Print the results\n","print(pred_class, prob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHZLZFDVzHqp","executionInfo":{"status":"ok","timestamp":1758769098792,"user_tz":-480,"elapsed":1183,"user":{"displayName":"ç‹çŸ¥æ–°","userId":"09870380652686636069"}},"outputId":"4cf88e79-0314-4aaf-e167-34a3463e0a82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] [[0.20458265 0.79541735]]\n"]}]}]}