{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqRkDoYu+GZB79EJ4vEUZw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["10 月 9 号\n","\n","##K Mean Clustering\n","\n","**简介：**\n","\n","1. **Clustering** is grouping things into “natural” categories when no class label is available. Labeling a large set of data samples can be **costly**.\n","\n","2. Clustering can be used for finding features that will be useful later for **categorization**.\n","\n","3. K-Means clustering is an unsupervised learning algorithm.The term ‘K’ is a number telling the system how many clusters you need to create. For example, if K = 3 refers to three clusters.\n","\n","Clustering 可以帮助我们分类，$K$表示我们将数据分成了几类\n","\n","##Method\n","\n","1. 选择一个$K$，作为seed。他们将成为我们的initial centroids（可以理解为中心）\n","2. 找到每一个data point离我们的$K$个centroid的距离\n","3. 将data point分类进最近的centroid的那一类\n","4. 重新计算我们的centroid。（求它们的重心即可）\n","5. 如果没有达到我们的convergence criterion，重复步骤2, 3, 4\n","\n","具体的实现：\n","\n","1. **Randomly Choose** $K$ data point as initial centroids\n","\\begin{align}\n","  1st-centroid: (70, 46, 56) \\\\\n","  2nd-centroid: (27, 67, 56) \\\\\n","  3rd-centroid: (37, 97, 32)\n","\\end{align}\n","2. Find the Distances Between Each Data Point with the 3 Centroids\n","\n","3. **Assign** Each Data Point to the closest centroid\n","\n","4. **Re-compute the Centroids** Using the Current Cluster Memberships\n","New 1st centroid:\n","\\begin{align}\n"," x_1 &= (67+35+60+65+49+70+70+57+68+65)/10 =60.6 \\\\\n"," x_2 &= (19+24+30+38+42+46+49+54+59+63)/10 =42.4 \\\\\n"," x_3 &= (14+35+4+35+52+56+55+51+55+52)/10=40.9   \\\\\n","\\end{align}\n"," New 2nd centroid:\n","\\begin{align}\n"," x_1 &= (19+23+27)/3 = 23   \\\\\n"," x_2 &= (15+62+67)/3 = 48   \\\\\n"," x_3 &= (39+41+56)/3 = 45.33\n","\\end{align}\n"," New 3rd centroid:\n","\\begin{align}\n"," x_1 &= (47+57+43+56+40+37+34)/7 =44.86     \\\\\n"," x_2 &= (71+75+78+79+87+97+103)/7 = 84.29    \\\\\n"," x_3 &= (9+5+17+35+13+32+23)/7=19.14\n","\\end{align}\n","\n","\n","##Convergence criterion\n","以下三者满足其一：\n","1. **No/Minimum re-assignments of data points** to different clusters\n","2. **No/Minimum change of centroids**\n","3. **Minimum decrease in the sum of squared error (SSE)** between successive  \n","  iteration\n","  \\begin{align}\n","  SSE = \\sum^{k}_{j=1} \\sum_{\\vec{x} \\in C_{j}} dist(\\vec{x}, \\vec{m_{j}})^{2}\n","  \\end{align}\n","  where: $C_{j}$ is the j-th cluster, $\\vec{m_j}$ is the centroid of cluster $C_j$, $dist(\\vec{x}, \\vec{m_j})$ is the distance between data point $\\vec{x}$ and centroid $\\vec{m_j}$\n","\n","##为什么这样会converge?\n","\n","我们需要证明，我们的上述方法，每次操作都可以让**SSE**变小\n","即：\n","\\begin{align}\n","\\sum^{m}_{i=1} ||x_i - x||^{2} \\geq \\sum^{m}_{i=1} ||x_i - \\overline{x}||^{2}\n","\\end{align}\n","\n","$Proof:$\n","\n","\\begin{align}\n","\\sum^{m}_{i=1} ||x_i - x||^{2} =\n","&\\sum^{m}_{i=1} ||(x_i - \\overline{x}) + (\\overline{x} - x)||^{2}\\\\ =\n","&\\sum^{m}_{i=1} (||x_i - \\overline{x}||^{2} + ||\\overline{x} - x||^{2} + 2(x_i - \\overline{x}) \\cdot (\\overline{x} - x)) \\\\ =\n","&\\sum^{m}_{i=1} ||x_i - \\overline{x}||^{2} + m||\\overline{x} - x||^{2} + 2\\sum^{m}_{i=1}(x_i - \\overline{x}) \\cdot (\\overline{x} - x) \\\\ =\n","&\\sum^{m}_{i=1} ||x_i - \\overline{x}||^{2} + m||\\overline{x} - x||^{2} - 2m\\overline{x} \\cdot (\\overline{x} - x) + 2(\\overline{x} - x) \\cdot \\sum^{m}_{i=1} x_i \\\\ =\n","&\\sum^{m}_{i=1} ||x_i - \\overline{x}||^{2} + m||\\overline{x} - x||^{2} \\\\ \\ge\n","&\\sum^{m}_{i=1} ||x_i - \\overline{x}||^{2}\n","\\end{align}\n","\n","Here, we obtain the second equation by using **Polarization law**:\n","\\begin{align}\n","  \\forall x, y \\in V , \\quad\n","  ||x + y||^{2} = ||x||^{2} + ||y||^{2} + 2Re⟨x, y⟩\n","\\end{align}\n","\n"],"metadata":{"id":"UlpfrPPM0IJx"}},{"cell_type":"markdown","source":["## Clustering Quantity\n","High quality clustering:\n","1. Maximizes inter-clusters distance (Isolation)\n","  i.e., distance between clusters\n","2. Minimizes intra-clusters distance (Compactness)\n","  i.e., distance between data points in the same cluster\n","\n","**remark:**\n","The quality of a clustering result depends on the **algorithm**, the **distance function**, and the **application**.\n","\n","##Weakness of K-Mean clustering\n","\n","It is **sensitive to outliers**\n","1. Outliers are data points that are very far away from other data points\n","2. Outliers could be errors in the data recording or some special data points with very different values\n","3. Desirable and undesirable clustering with outliers\n","\n","The algorithm is **sensitive to initial seeds**.\n","\n","The K-Means algorithm is **not suitable for discovering clusters that are not hyper-ellipsoids** (or hyper-spheres).\n","\n"],"metadata":{"id":"7Xs6Hbqy37lx"}},{"cell_type":"markdown","source":["##Pros and Cons\n","\n","Pros:\n","1. Easy to understand and implement\n","2. it is efficient, given K and the number of iterations is small\n","\n","Cons:\n","1. It is only applicable if the mean is defined\n","2. For categorical data, K model is used (hamming distance and majority vote)\n","3. The user need to specify K\n","4. It is sensitive to outliers\n","\n","##Summary\n","**Despite weakness, K-Means is still the most popuplar algorithm due to its simplicity, effciency.**"],"metadata":{"id":"rRx80Wjpj_uH"}},{"cell_type":"markdown","source":["##Remark: Dimension Reduction using PCA\n","Principal Component Analysis is one of the most frequently used method for **dimension reduction**. projecting data onto its orthogonal feature subspace.\n","\n","Basically PCA is nothing else but a **projection** of some higher dimensional data into a lower dimension.\n","\n","Mathematically this can be done by calculating the so called **eigenvectors of the covariance matrix**, after that we just use the n eigenvectors with the biggest eigenvalues to project the object into n-dimensional space.\n","\n"],"metadata":{"id":"RefX2hZNl4D1"}}]}