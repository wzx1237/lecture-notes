{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIMCpjYnWD8jPfz1KkJvAa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["9æœˆ24æ—¥\n","**KNN**\n","\n","ä»€ä¹ˆæ˜¯**KNN**ï¼Ÿ\n","KNN: k-nearest neighborhood\n","\n","KNN is a **lazy learning**, **non-parametric algorithm**, as it does not\n","make any assumptions on the data being studied\n","\n","KNN no learning at all!\n","\n","**KNNçš„å·¥ä½œæ–¹å¼ï¼š**\n","1. å‡†å¤‡æ•°æ®ï¼Œå°†æ•°æ®å˜æˆä¸€ä¸ªä¸ªå‘é‡\n","2. æ‰¾åˆ°ä¸æˆ‘ä»¬çš„test dataè·ç¦»æœ€è¿‘çš„Kä¸ªæ•°æ®ã€‚è¿™ä¸ªè·ç¦»å¯ä»¥æ˜¯æœ€ç®€å•çš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–çš„è·ç¦»ï¼Œæ¯”å¦‚æ›¼å“ˆé¡¿è·ç¦»ç­‰\n","3. åœ¨è¿™æœ€è¿‘çš„Kä¸ªæ•°æ®ä¸­ï¼Œé€šè¿‡majority voteçš„æ–¹å¼æ‰¾å‡ºæˆ‘ä»¬çš„test dataåº”è¯¥å±äºå“ªä¸€ç±»ã€‚å°†å®ƒå½’ç±»åï¼Œè¿”å›è¿™ä¸€ç±»æ•°æ®åº”è¯¥å¯¹åº”çš„label\n","\n","ä½†æ˜¯åœ¨å‡†å¤‡æ•°æ®çš„æ—¶å€™æœ‰ä¸€ç‚¹è¦æ³¨æ„ï¼š\n","ä¸åŒç±»å‹çš„æ•°æ®å•ä½ä¸åŒï¼Œå› æ­¤åœ¨è®¡ç®—è·ç¦»çš„æ—¶å€™æƒé‡ä¸åŒï¼Œä½†æˆ‘ä»¬æƒ³è¦è®©ä»–ä»¬æœ‰ä¸€æ ·çš„æƒé‡ï¼Œæ€ä¹ˆåŠï¼Ÿ\n","\n","**Standardization:**\n","\\begin{align}\n"," X = \\frac{X - X_{mean}}{\\text{standard deviation}}\n","\\end{align}\n","\\begin{align}\n"," \\text{standard deviation} = \\sqrt{\\frac{\\sum^{N}_{j=1} (X_{i}-X_{mean})^2}{N}}\n","\\end{align}\n","åœ¨é€‰æ‹©è·ç¦»çš„è®¡ç®—æ–¹å¼ä¸Šæœ‰ä¸€äº›æŠ€å·§ï¼š\n","1. æ›¼å“ˆé¡¿è·ç¦»ï¼šé€‚åˆçº¿æ€§å…³ç³»å’Œç¦»æ•£ç‰¹å¾ã€‚é€‚åˆäºåœ¨é«˜ç»´ç©ºé—´è®¡ç®—ã€‚\n","2. æ¬§å‡ é‡Œå¾—è·ç¦»ï¼šé€‚åˆè¿ç»­ç‰¹å¾ï¼Œè®¡ç®—çœŸå®å‡ ä½•è·ç¦»ã€‚\n","3. ä½™å¼¦è·ç¦»ï¼šé€‚åˆæ–‡æœ¬å’Œé«˜ç»´ç¨€ç–æ•°æ®ï¼Œå…³æ³¨æ–¹å‘ã€‚\n","\n","   $cosine \\quad distiance:$\n","\\begin{align}\n","   \\cos\\beta = \\frac{\\vec{X} \\cdot \\vec{X_{test}}}{\\left|X\\right| \\times \\left|X_{test}\\right|}\n","   = \\frac{\\sum^{n}_{i=1}x_{i}^{train} \\times x_{i}^{test}}\n","   {\\sqrt{\\sum^{n}_{i=1} (x_{i}^{train})^{2}} \\sqrt{\\sum^{n}_{i=1}(x_{i}^{test})^{2}}}\n","\\end{align}\n","\\begin{align}\n","  distance = 1 - \\cos\\beta\n","\\end{align}\n","\n","4. æ±‰æ˜è·ç¦»ï¼šé€‚åˆäºŒè¿›åˆ¶æˆ–ç±»åˆ«ç‰¹å¾ï¼Œæ¯”è¾ƒç›¸åŒé•¿åº¦çš„å­—ç¬¦ä¸²ã€‚\n","   æ¯”è¾ƒå­—ç¬¦ä¸²ä¸­æœ‰å¤šå°‘ä¸ªä¸åŒçš„å­—ç¬¦\n","\n","**åœ¨ä¼˜åŒ–æˆ‘ä»¬çš„KNNæ—¶ï¼Œæœ‰å‡ ä¸ªä¸»è¦çš„æ–¹å‘ï¼š**\n","   1. Kçš„å–å€¼\n","   2. è·ç¦»çš„å…¬å¼é€‰æ‹©\n","   3. model evaluation\n","\n"," å…³äºKçš„å–å€¼ï¼š\n"," æ–¹æ³•ä¸€ï¼šRule of Thumb\n","\\begin{align}\n","    K = \\sqrt{N}\n","\\end{align}\n","åŸå› ï¼š\n","\n","**Using Cross-validation to Estimate K**\n","\n","æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®åˆ†æˆ$d$å †ï¼Œå–$d-1$å †è®­ç»ƒæ¨¡å‹ï¼Œå‰©ä¸‹çš„ä¸€ä¸ªä½œä¸ºæµ‹è¯•é›†\n","\n","è¿™æ ·ä¸€ä¸ªä¸€ä¸ªæµ‹è¯•å¯¹åº”çš„$d$, æ‰¾åˆ°æœ€å¥½ç”¨çš„é‚£ä¸€ä¸ª$K$å°±è¡Œ\n","\n","æœ€åçš„erroræ˜¯æˆ‘ä»¬æ¯ä¸€æ¬¡è®­ç»ƒçš„errorçš„å¹³å‡å€¼ï¼Œ å³ï¼š\n","$E = \\sum_{i=1}^{n} E_{i}$\n","\n","è¿™é‡Œï¼Œ$E_{i}$ä»£è¡¨ç¬¬$i$æ¬¡è®­ç»ƒçš„error, $E$æ˜¯æ€»error\n","\n","è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬çš„é—®é¢˜å°±è½¬å˜æˆäº†ï¼šå¦‚ä½•è¡¡é‡ä¸€ä¸ª$K$æ˜¯ä¸æ˜¯å¥½çš„\n","\n","**Model Evaluation:**\n","1. confusion matrix\n","2. Precision, Recall, and F1 score\n","3. MCC\n","\n","æˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ä¸ª2*2çš„confusion matrixæ¥åšbinary classification\n","\n","å®ƒè¢«åˆ†æˆäº†å››éƒ¨åˆ†ï¼š\n","\n","\\begin{matrix}\n","        &Positive &Negative \\\\\n","Positive &TP & FN \\\\\n","Negative &FP & TN\n","\\end{matrix}\n","\n","TPä»£è¡¨ï¼šTrue positiveï¼Œè¯´æ˜æ¨¡å‹æ­£ç¡®çš„å°†positive classé¢„æµ‹ä¸ºpositive\n","\n","TNä»£è¡¨ï¼šTrue negativeï¼Œè¯´æ˜æ¨¡å‹æ­£ç¡®çš„å°†negative classé¢„æµ‹ä¸ºnegative\n","\n","FPä»£è¡¨ï¼šFalse positiveï¼Œè¯´æ˜æ¨¡å‹é”™è¯¯çš„å°†negative classé¢„æµ‹ä¸ºpositive\n","\n","FNä»£è¡¨ï¼šFalse negativeï¼Œè¯´æ˜æ¨¡å‹é”™è¯¯çš„å°†positive classé¢„æµ‹ä¸ºnegative\n","\n","å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªmatrixå†™å‡ºerrorå’Œaccuracyçš„å…¬å¼ï¼š\n","\n","\\begin{align}\n","  Error = \\frac{FP + FN}{FP + FN + TP + TN}\n","     = 1 - Accuracy\n","  \\\\\n","  Accuracy = \\frac{TP + TN}{FP + FN + TP + TN}\n","       = 1 - Error\n","\\end{align}\n","å› æ­¤ï¼Œprecisionè¢«å®šä¹‰ä¸ºï¼šwhat fraction of predicts as a positive class were actually positive.\n","\\begin{align}\n","  Precision = \\frac{TP}{TP + FP}\n","\\end{align}\n","**Precision** tells us how much we can trust the model when it predicts an individual sample as **positive**\n","\n","Recallè¢«å®šä¹‰ä¸ºï¼šwhat fraction of positive samples were correctly predicted as positive\n","\\begin{align}\n","  Recall = \\frac{TP}{TP + FN}\n","\\end{align}\n","**Recall** measures the ability of the model to find all the **positive** in the dataset\n","\n","ä½†æ˜¯å½“æˆ‘ä»¬å°†æ‰€æœ‰çš„sampleéƒ½åˆ’åˆ†ä¸ºTæ—¶ï¼Œ$Recall = 1$, è¿™ä¸ªæ²¡æœ‰æ„ä¹‰ã€‚\n","\n","\n","**ä»€ä¹ˆåˆæ˜¯F1-Score?**\n","\n","**F1-score: It combines precision and recall into a single measure**\n","\n","ä»æ•°å­¦è¡¨è¾¾å¼ä¸Šæ¥è¯´ï¼ŒF1-Scoreæ˜¯Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡å€¼\n","\\begin{align}\n","  F1-score &= \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} \\\\\n","       &= \\frac{2 \\times Precision \\times Recall}{Precision + Recall} \\\\\n","       &= \\frac{2TP}{2TP + FP + FN}\n","\\end{align}\n","å½“$F1-score = 1$æ—¶ï¼Œè¯´æ˜æˆ‘ä»¬çš„æ¨¡å‹éå¸¸å®Œç¾\n","\n","**Matthewâ€™s Correlation Coefficient Formula(MCC):**\n","\n","is a best single-value classification metric(è¡¡é‡æ ‡å‡†)ï¼Œ\n","\n","which helps to summarize the confusion matrix or an error matrix.\n","\n","\\begin{align}\n","  MCC = \\frac{TN \\times TP - FN \\times FP}\n","  {\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n","\\end{align}\n","å½“MCC = 1æ—¶ï¼Œè¯´æ˜æˆ‘ä»¬çš„é¢„æµ‹ååˆ†å®Œç¾\n","\n","å½“MCC = 0æ—¶ï¼Œè¯´æ˜æˆ‘ä»¬çš„é¢„æµ‹å®Œå…¨æ²¡ç”¨\n","\n","å½“MCC = -1æ—¶ï¼Œè¯´æ˜æˆ‘ä»¬çš„æ¯ä¸€ä¸ªé¢„æµ‹éƒ½æ˜¯é”™çš„\n","\n","å¦‚æœæˆ‘ä»¬æœ‰æ›´å¤šçš„classesï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠï¼Ÿ\n","**Confusion matrix for multi-class Classification**\n","\n","ä¾‹å­ï¼š\n","\\begin{matrix}\n","    & apple & orange & mango \\\\\n","apple & 7 & 8 & 9 \\\\\n","orange & 1 & 2 & 3 \\\\\n","mango & 3 & 2 & 1\n","\\end{matrix}\n","å¯¹äºè‹¹æœè€Œè¨€ï¼š\n","\n","$TP = 7$, $TN = 1 + 2 + 2 + 3 = 8$, $FN = 8 + 9 = 17$, $FP = 1 + 3 = 4$\n","\n","**Micro-F1**\n","\n"," It is calculated by considering the total TP, total FP and total FN of the model.\n","\n"," It does not consider each class individually, it calculates the metrics globally.\n","\n","For our example:\n","\\begin{align}\n"," &(Total)TP = 7+2+1 = 10 \\\\\n"," &(Total)FP = (8+9) + (1 + 3) + (3 + 2) = 26 \\\\\n"," &(Total)FN = (1+3) + (8 + 2) + (9+3) = 26 \\\\\n"," &Precision = 10/(10 + 26) = 0.28 \\\\\n"," &Recall = 10/(10+26) = 0.28 \\\\\n"," &(Micro)F1 =\n"," \\frac{2TP} {2TP+FP+FN} = \\frac{2(10)}{2(10)+26+26} = 0.28\n","\\end{align}\n"," Now, you can see that we are calculating the metrics globally, all the\n"," measures become equal:\n","\n"," $Precision = Recall = Micro F1 = Accuracy$\n","\n","**Macro F1:**\n","\n","**Macro F1** is also called **macro-averaged F1-score**.\n"," It calculates metrics for each class individually and then take unweighted mean of the measures.\n","\n"," According to the data in the table shown earlier:\n"," \\begin{align}\n"," Appleâ€™s \\quad F1-score = 0.40 \\\\\n"," Orangeâ€™s \\quad F1-score = 0.22 \\\\\n"," Mangoâ€™s \\quad F1-score = 0.11 \\\\\n"," \\end{align}\n"," Hence, $macro F1 = (0.40 + 0.22 + 0.11)/3 = 0.24$\n","\n","\n","**Error Measurement for Regression Problems**\n","\n","Mean Absolute Error(MAE):\n","\\begin{align}\n","  MAE = \\frac{\\sum^{m}_{i=1} |{a_{i} - p_{i}|}}{m}\n","\\end{align}\n","Mean Square Error (MSE):\n","\\begin{align}\n","  MSE = \\frac{\\sum^{m}_{i=1} (a_{i} - p_{i})^{2}}{m}\n","\\end{align}\n","Mean Absolute Percentage Error (MAPE):\n","\\begin{align}\n","  MAPE = \\frac{\\sum^{m}_{i=1} |\\frac{a_{i} - p_{i}}{a_{i}}|}{m}\n","\\end{align}\n"],"metadata":{"id":"UBXECZpd9XxC"}},{"cell_type":"markdown","source":["##Choose K\n","why do we choose $K = \\sqrt{N}$ ?\n","\n","Theorem 11.1 (Devroye and Gyorfi (1985), Zhao (1987))\n","Assume that each $ğœ‡$ has a density. If $K \\rightarrow \\infty$ and $\\frac{K}{N} \\rightarrow 0$ then for every $\\epsilon > 0$ there is an $N_0$ such that for $N > N_0$,\n","\\begin{align}\n","    P(L_N âˆ’L^âˆ— > Ïµ) â‰¤ 2e^{âˆ’NÏµ^{2}/(72Î³_{d}^{2})}\n","\\end{align}\n","where the $Î³_{d}$ is the minimal number of cones centered at the origin of angle $\\frac{Ï€}{6}$ that cover $â„$. Thus, the KNN rule is strongly consistent.\n","\n","$L_N =L_N(g_N) = P(g_N(X)= Y)$ is the risk of the K-nearest neighbor classifier $g_N(X)$, where $g_N$ is estimated from a sample of size $N$.\n","\n","$L^âˆ— = L(g^x) = inf_{gâˆˆG}P(g(X)= Y)$, is the Bayes-optimal classification risk, or Bayes error rate, that is the risk of the Bayes classifier $g^âˆ—$. Glossing over the measure theoretic technicalities, that the measure $ğœ‡$ has a density just means that $X$ has a density.\n","\n","Parsing the theorem, the **main condition requires that $K â†’ âˆ$ as the sample size $N â†’ âˆ$ in such a way that $\\frac{K}{N} â†’ 0$.** Your heuristic satisfies this condition because $K =âŒŠ\\sqrt{N}âŒ‹â†’âˆ$ and $\\frac{K}{N} = \\frac{âŒŠ\\sqrt{N}âŒ‹}{N} â‰ˆ \\frac{1}{\\sqrt{N}}â†’0$."],"metadata":{"id":"F9m3FhIVrHzv"}},{"cell_type":"markdown","source":["##Supplymentary Notes:\n","\n","**Weighted F1:**\n","\n"," Weighted F1 is also called weighted-averaged F1-score. Unlike Macro F1, it takes a weighted mean of the measures, The **weights for each class** are the **total number of samples of that class**.\n","\n"," Since we had 11 Apples, 12 Oranges, and 13 Mangoes, so\n"," $Weighted-F1 = \\frac{(0.4âˆ—11)(0.22âˆ—12)+(0.11âˆ—13)}{11+12+13} = 0.24$"],"metadata":{"id":"TLTtgHk303tO"}}]}